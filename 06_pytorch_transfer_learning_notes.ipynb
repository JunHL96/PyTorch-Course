{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvS-LJha5Ajy"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1mTdK6FC9U2a0gc673VZKrAg26g0RfDj9\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "[View Source Code](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/06_pytorch_transfer_learning.ipynb) | [View Slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/06_pytorch_transfer_learning.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_K-Y9qL5Ajz"
      },
      "source": [
        "# 06. PyTorch Transfer Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaGP112c5Ajz"
      },
      "source": [
        "\n",
        "> **Note:** This notebook uses `torchvision`'s new [multi-weight support API (available in `torchvision` v0.13+)](https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/).\n",
        "\n",
        "We've built a few models by hand so far.\n",
        "\n",
        "But their performance has been poor.\n",
        "\n",
        "You might be thinking, **is there a well-performing model that already exists for our problem?**\n",
        "\n",
        "And in the world of deep learning, the answer is often *yes*.\n",
        "\n",
        "We'll see how by using a powerful technique called [**transfer learning**](https://developers.google.com/machine-learning/glossary#transfer-learning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tgc-Dme5Ajz"
      },
      "source": [
        "## What is transfer learning?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NyMovo-5Ajz"
      },
      "source": [
        "\n",
        "**Transfer learning** allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.\n",
        "\n",
        "For example, we can take the patterns a computer vision model has learned from datasets such as [ImageNet](https://www.image-net.org/) (millions of images of different objects) and use them to power our FoodVision Mini model.\n",
        "\n",
        "Or we could take the patterns from a [language model](https://developers.google.com/machine-learning/glossary#masked-language-model) (a model that's been through large amounts of text to learn a representation of language) and use them as the basis of a model to classify different text samples.\n",
        "\n",
        "The premise remains: find a well-performing existing model and apply it to your own problem.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-transfer-learning-example-overview.png\" alt=\"transfer learning overview on different problems\" width=900/>\n",
        "\n",
        "*Example of transfer learning being applied to computer vision and natural language processing (NLP). In the case of computer vision, a computer vision model might learn patterns on millions of images in ImageNet and then use those patterns to infer on another problem. And for NLP, a language model may learn the structure of language by reading all of Wikipedia (and perhaps more) and then apply that knowledge to a different problem.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KH-PU3w5Aj0"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Ao3v885Aj0"
      },
      "source": [
        "## Why use transfer learning?\n",
        "\n",
        "There are two main benefits to using transfer learning:\n",
        "\n",
        "1. Can leverage an existing model (usually a neural network architecture) proven to work on problems similar to our own.\n",
        "2. Can leverage a working model which has **already learned** patterns on similar data to our own. This often results in achieving **great results with less custom data**.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-transfer-learning-for-foodvision-mini%20.png\" alt=\"transfer learning applied to FoodVision Mini\" width=900/>\n",
        "\n",
        "*We'll be putting these to the test for our FoodVision Mini problem, we'll take a computer vision model pretrained on ImageNet and try to leverage its underlying learned representations for classifying images of pizza, steak and sushi.*\n",
        "\n",
        "Both research and practice support the use of transfer learning too.\n",
        "\n",
        "A finding from a recent machine learning research paper recommended practitioners use transfer learning wherever possible.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-how-to-train-your-vit-section-6-transfer-learning-highlight.png\" width=900 alt=\"how to train your vision transformer paper section 6, advising to use transfer learning if you can\"/>\n",
        "\n",
        "*A study into the effects of whether training from scratch or using transfer learning was better from a practitioner's point of view, found transfer learning to be far more beneficial in terms of cost and time. **Source:** [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers](https://arxiv.org/abs/2106.10270) paper section 6 (conclusion).*\n",
        "\n",
        "And Jeremy Howard (founder of [fastai](https://www.fast.ai/)) is a big proponent of transfer learning.\n",
        "\n",
        "> The things that really make a difference (transfer learning), if we can do better at transfer learning, it’s this world changing thing. Suddenly lots more people can do world-class work with less resources and less data. — [Jeremy Howard on the Lex Fridman Podcast](https://youtu.be/Bi7f1JSSlh8?t=72)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlLrL7QU5Aj0"
      },
      "source": [
        "## Where to find pretrained models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u-qqu5A5Aj0"
      },
      "source": [
        "\n",
        "The world of deep learning is an amazing place.\n",
        "\n",
        "So amazing that many people around the world share their work.\n",
        "\n",
        "Often, code and pretrained models for the latest state-of-the-art research is released within a few days of publishing.\n",
        "\n",
        "And there are several places you can find pretrained models to use for your own problems.\n",
        "\n",
        "| **Location** | **What's there?** | **Link(s)** |\n",
        "| ----- | ----- | ----- |\n",
        "| **PyTorch domain libraries** | Each of the PyTorch domain libraries (`torchvision`, `torchtext`) come with pretrained models of some form. The models there work right within PyTorch. | [`torchvision.models`](https://pytorch.org/vision/stable/models.html), [`torchtext.models`](https://pytorch.org/text/main/models.html), [`torchaudio.models`](https://pytorch.org/audio/stable/models.html), [`torchrec.models`](https://pytorch.org/torchrec/torchrec.models.html) |\n",
        "| **HuggingFace Hub** | A series of pretrained models on many different domains (vision, text, audio and more) from organizations around the world. There's plenty of different datasets too. | https://huggingface.co/models, https://huggingface.co/datasets |\n",
        "| **`timm` (PyTorch Image Models) library** | Almost all of the latest and greatest computer vision models in PyTorch code as well as plenty of other helpful computer vision features. | https://github.com/rwightman/pytorch-image-models|\n",
        "| **Paperswithcode** | A collection of the latest state-of-the-art machine learning papers with code implementations attached. You can also find benchmarks here of model performance on different tasks. | https://paperswithcode.com/ |\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-transfer-learning-where-to-find-pretrained-models.png\" alt=\"different locations to find pretrained neural network models\" width=900/>\n",
        "\n",
        "*With access to such high-quality resources as above, it should be common practice at the start of every deep learning problem you take on to ask, \"Does a pretrained model exist for my problem?\"*\n",
        "\n",
        "> **Exercise:** Spend 5-minutes going through [`torchvision.models`](https://pytorch.org/vision/stable/models.html) as well as the [HuggingFace Hub Models page](https://huggingface.co/models), what do you find? (there's no right answers here, it's just to practice exploring)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgzkKCBo5Aj0"
      },
      "source": [
        "## What we're going to cover\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koAZPeHS5Aj0"
      },
      "source": [
        "\n",
        "We're going to take a pretrained model from `torchvision.models` and customise it to work on (and hopefully improve) our FoodVision Mini problem.\n",
        "\n",
        "| **Topic** | **Contents** |\n",
        "| ----- | ----- |\n",
        "| **0. Getting setup** | We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. |\n",
        "| **1. Get data** | Let's get the pizza, steak and sushi image classification dataset we've been using to try and improve our model's results. |\n",
        "| **2. Create Datasets and DataLoaders** | We'll use the `data_setup.py` script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders. |\n",
        "| **3. Get and customise a pretrained model** | Here we'll download a pretrained model from `torchvision.models` and customise it to our own problem. |\n",
        "| **4. Train model** | Let's see how the new pretrained model goes on our pizza, steak, sushi dataset. We'll use the training functions we created in the previous chapter. |\n",
        "| **5. Evaluate the model by plotting loss curves** | How did our first transfer learning model go? Did it overfit or underfit?  |\n",
        "| **6. Make predictions on images from the test set** | It's one thing to check out a model's evaluation metrics but it's another thing to view its predictions on test samples, let's *visualize, visualize, visualize*! |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hknr9kJ5Aj0"
      },
      "source": [
        "## Where can you get help?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iuD_voQ5Aj0"
      },
      "source": [
        "\n",
        "All of the materials for this course [are available on GitHub](https://github.com/mrdbourke/pytorch-deep-learning).\n",
        "\n",
        "If you run into trouble, you can ask a question on the course [GitHub Discussions page](https://github.com/mrdbourke/pytorch-deep-learning/discussions).\n",
        "\n",
        "And of course, there's the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) and [PyTorch developer forums](https://discuss.pytorch.org/), a very helpful place for all things PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BxpyByC5Aj0"
      },
      "source": [
        "## 0. Getting setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fftlWZR35Aj1"
      },
      "source": [
        "\n",
        "Let's get started by importing/downloading the required modules for this section.\n",
        "\n",
        "To save us writing extra code, we're going to be leveraging some of the Python scripts (such as `data_setup.py` and `engine.py`) we created in the previous section, [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/).\n",
        "\n",
        "Specifically, we're going to download the [`going_modular`](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/going_modular) directory from the `pytorch-deep-learning` repository (if we don't already have it).\n",
        "\n",
        "We'll also get the [`torchinfo`](https://github.com/TylerYep/torchinfo) package if it's not available.\n",
        "\n",
        "`torchinfo` will help later on to give us a visual representation of our model.\n",
        "\n",
        "> **Note:** As of June 2022, this notebook uses the nightly versions of `torch` and `torchvision` as `torchvision` v0.13+ is required for using the updated multi-weights API. You can install these using the command below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVQDHd525Aj1",
        "outputId": "fb2ec95a-95eb-4456-ae46-769ba16ec2b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "torch version: 2.5.1+cu121\n",
            "torchvision version: 0.20.1+cu121\n"
          ]
        }
      ],
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG3D5aQJ5Aj1"
      },
      "source": [
        "#### Install the required packages and download the `going_modular` directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ziHL2Ab05Aj1"
      },
      "outputs": [],
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular import data_setup, engine\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n_5mk7I5Aj1"
      },
      "source": [
        "- **Git Clone**:  \n",
        "  - `git clone` is a command used to copy a repository from a remote source (like GitHub) to your local machine.  \n",
        "  - In this code, `!git clone https://github.com/mrdbourke/pytorch-deep-learning` downloads the entire `pytorch-deep-learning` repository from GitHub.  \n",
        "  - The `!` at the beginning of the command allows it to be executed directly in a Jupyter notebook cell as a shell command.\n",
        "\n",
        "- **MV (Move)**:  \n",
        "  - The `mv` command is used to move or rename files and directories in Unix-like systems.  \n",
        "  - In this code, `!mv pytorch-deep-learning/going_modular .` moves the `going_modular` directory from the downloaded `pytorch-deep-learning` folder to the current directory (`.`).  \n",
        "  - This makes the `going_modular` scripts easily accessible for import in the notebook.\n",
        "\n",
        "- **RM (Remove)**:  \n",
        "  - The `rm` command is used to delete files or directories.  \n",
        "  - The `-rf` flags are used to forcefully remove directories and their contents recursively.  \n",
        "  - In this code, `!rm -rf pytorch-deep-learning` deletes the `pytorch-deep-learning` directory after the `going_modular` folder has been moved.  \n",
        "  - This cleans up the workspace by removing unnecessary files downloaded during the `git clone` process.\n",
        "\n",
        "- **Summary**:  \n",
        "  - `git clone` downloads the repository.  \n",
        "  - `mv` moves the required directory to the current location.  \n",
        "  - `rm -rf` removes the downloaded repository to keep the workspace tidy.  \n",
        "  - This sequence ensures the necessary scripts are available locally while maintaining a clean workspace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDUMmOaO5Aj1"
      },
      "source": [
        "#### Setup Device-agnostic code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKBtZNOD5Aj1",
        "outputId": "6b0ed1a3-f706-4d76-8220-49fc0aa0ef17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "  # Setup device-agnostic code\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\" # NVIDIA GPU\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\" # Apple GPU\n",
        "else:\n",
        "    device = \"cpu\" # Defaults to CPU if NVIDIA GPU/Apple GPU aren't available\n",
        "\n",
        "\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oLRm2kf5Aj2"
      },
      "source": [
        "#### Setup `.py` scripts for reuse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_6OffwH5Aj2",
        "outputId": "cb8563ff-42b8-4071-e89d-06c35d5d09b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/going_modular\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the 'scripts' directory to the Python path\n",
        "sys.path.append(os.path.abspath(\"going_modular\"))\n",
        "\n",
        "# from going_modular import data_setup, engine, model_builder, train, utils\n",
        "\n",
        "print(os.path.abspath(\"going_modular\")) # Verify the path is correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7ZUVZbi5Aj2"
      },
      "source": [
        "## 1. Get data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqib86Mm5Aj2"
      },
      "source": [
        "\n",
        "Before we can start to use **transfer learning**, we'll need a dataset.\n",
        "\n",
        "To see how transfer learning compares to our previous attempts at model building, we'll download the same dataset we've been using for FoodVision Mini.\n",
        "\n",
        "Let's write some code to download the [`pizza_steak_sushi.zip`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi.zip) dataset from the course GitHub and then unzip it.\n",
        "\n",
        "We can also make sure if we've already got the data, it doesn't redownload."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKXyMP2D5Aj2",
        "outputId": "659fe0fe-11c8-4d6f-bb4f-2b9bb467233c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/pizza_steak_sushi directory exists.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download pizza, steak, sushi data\n",
        "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "        print(\"Downloading pizza, steak, sushi data...\")\n",
        "        f.write(request.content)\n",
        "\n",
        "    # Unzip pizza, steak, sushi data\n",
        "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "        print(\"Unzipping pizza, steak, sushi data...\")\n",
        "        zip_ref.extractall(image_path)\n",
        "\n",
        "    # Remove .zip file\n",
        "    os.remove(data_path / \"pizza_steak_sushi.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7871GkO5Aj2"
      },
      "source": [
        "Excellent!\n",
        "\n",
        "Now we've got the same dataset we've been using previously, a series of images of pizza, steak and sushi in standard image classification format.\n",
        "\n",
        "Let's now create paths to our training and test directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OjzxLRng5Aj2"
      },
      "outputs": [],
      "source": [
        "# Setup Dirs\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gry9WJRI5Aj2"
      },
      "source": [
        "## 2. Create Datasets and DataLoaders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc0_XWa95Aj2"
      },
      "source": [
        "\n",
        "Since we've downloaded the `going_modular` directory, we can use the [`data_setup.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/data_setup.py) script we created in section [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy) to prepare and setup our DataLoaders.\n",
        "\n",
        "But since we'll be using a pretrained model from [`torchvision.models`](https://pytorch.org/vision/stable/models.html), there's a specific transform we need to prepare our images first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDTT3dI35Aj2"
      },
      "source": [
        "### 2.1 Creating a transform for `torchvision.models` (manual creation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXTFumkl5Aj2"
      },
      "source": [
        "#### Understanding Data Transforms for Pretrained Models in PyTorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmVKJFcn5Aj2"
      },
      "source": [
        "\n",
        "When using a pretrained model, it's important that **your custom data going into the model is prepared in the same way as the original training data that went into the model**. This ensures that the model can make accurate predictions, as it has been trained on data with specific characteristics.\n",
        "\n",
        "Prior to `torchvision` v0.13+, to create a transform for a pretrained model in `torchvision.models`, the documentation stated:\n",
        "\n",
        "> All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.\n",
        ">\n",
        "> The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`.\n",
        ">\n",
        "> You can use the following transform to normalize:\n",
        ">\n",
        "> ```\n",
        "> normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        ">                                  std=[0.229, 0.224, 0.225])\n",
        "> ```\n",
        "\n",
        "The good news is, we can achieve the above transformations with a combination of:\n",
        "\n",
        "| **Transform number** | **Transform required** | **Code to perform transform** |\n",
        "| ----- | ----- | ----- |\n",
        "| 1 | Mini-batches of size `[batch_size, 3, height, width]` where height and width are at least 224x224^. | `torchvision.transforms.Resize()` to resize images into `[3, 224, 224]`^ and `torch.utils.data.DataLoader()` to create batches of images. |\n",
        "| 2 | Values between 0 & 1. | `torchvision.transforms.ToTensor()` |\n",
        "| 3 | A mean of `[0.485, 0.456, 0.406]` (values across each colour channel). | `torchvision.transforms.Normalize(mean=...)` to adjust the mean of our images.  |\n",
        "| 4 | A standard deviation of `[0.229, 0.224, 0.225]` (values across each colour channel). | `torchvision.transforms.Normalize(std=...)` to adjust the standard deviation of our images.  |\n",
        "\n",
        "> **Note:** some pretrained models from `torchvision.models` in different sizes to `[3, 224, 224]`, for example, some might take them in `[3, 240, 240]`. For specific input image sizes, see the documentation.\n",
        "\n",
        "---\n",
        "\n",
        "##### Why These Specific Numbers?\n",
        "\n",
        "The mean and standard deviation values (`[0.485, 0.456, 0.406]` for mean and `[0.229, 0.224, 0.225]` for standard deviation) might seem arbitrary, but they are not. These values were calculated from the **ImageNet dataset**, which is a large dataset of over 14 million images across 1,000 classes. Here's how they were derived:\n",
        "\n",
        "1. **Mean Calculation**:\n",
        "   - The mean values `[0.485, 0.456, 0.406]` represent the average pixel intensity across the red, green, and blue (RGB) channels of the ImageNet dataset.\n",
        "   - These values were computed by taking the average pixel intensity for each channel across a large subset of the ImageNet dataset.\n",
        "\n",
        "2. **Standard Deviation Calculation**:\n",
        "   - The standard deviation values `[0.229, 0.224, 0.225]` represent the spread or variability of pixel intensities across the RGB channels.\n",
        "   - These values were computed by calculating the standard deviation of pixel intensities for each channel across the same subset of the ImageNet dataset.\n",
        "\n",
        "---\n",
        "\n",
        "##### Why Do We Need to Normalize Data?\n",
        "\n",
        "Normalizing the data (subtracting the mean and dividing by the standard deviation) has several benefits:\n",
        "\n",
        "1. **Faster Convergence**:\n",
        "   - Normalizing the data helps the neural network converge faster during training. This is because the optimization algorithm (e.g., gradient descent) can navigate the loss landscape more efficiently when the input data is centered around zero and has a consistent scale.\n",
        "\n",
        "2. **Better Performance**:\n",
        "   - Pretrained models are trained on normalized data, so feeding them normalized data ensures that the model performs as expected. If the input data is not normalized, the model's predictions may be inaccurate.\n",
        "\n",
        "3. **Stability**:\n",
        "   - Normalization helps stabilize the training process by preventing large variations in the input data, which can lead to numerical instability or exploding gradients.\n",
        "\n",
        "---\n",
        "\n",
        "##### Do We Always Need to Normalize?\n",
        "\n",
        "While normalization is highly recommended, it is not strictly necessary. Neural networks are capable of learning the appropriate data distributions on their own. However, providing normalized data can significantly speed up training and improve model performance, especially when using pretrained models.\n",
        "\n",
        "---\n",
        "\n",
        "##### Customizing Transforms for Your Data\n",
        "\n",
        "If you're working with a dataset other than ImageNet, you can calculate your own mean and standard deviation values. Here's how:\n",
        "\n",
        "1. **Calculate Mean and Standard Deviation**:\n",
        "   - Iterate through your dataset and compute the mean and standard deviation for each channel (RGB) across all images.\n",
        "   - Use these values to create a custom normalization transform.\n",
        "\n",
        "2. **Apply Custom Transforms**:\n",
        "   - Replace the ImageNet mean and standard deviation values with your custom values in the `transforms.Normalize()` function.\n",
        "\n",
        "For example:\n",
        "```python\n",
        "custom_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[your_mean_r, your_mean_g, your_mean_b],\n",
        "                         std=[your_std_r, your_std_g, your_std_b])\n",
        "])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xN7kvLWs5Aj2"
      },
      "outputs": [],
      "source": [
        "# Create a transforms pipeline manually (required for torchvision < 0.13)\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czi6fZ3f5Aj2"
      },
      "source": [
        "Wonderful!\n",
        "\n",
        "Now we've got a **manually created series of transforms** ready to prepare our images, let's create training and testing DataLoaders.\n",
        "\n",
        "We can create these using the `create_dataloaders` function from the [`data_setup.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/data_setup.py) script we created in [05. PyTorch Going Modular Part 2](https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy).\n",
        "\n",
        "We'll set `batch_size=32` so our model sees mini-batches of 32 samples at a time.\n",
        "\n",
        "And we can transform our images using the transform pipeline we created above by setting `transform=manual_transforms`.\n",
        "\n",
        "> **Note:** I've included this manual creation of transforms in this notebook because you may come across resources that use this style. It's also important to note that because these transforms are manually created, they're also infinitely customizable. So if you wanted to include data augmentation techniques in your transforms pipeline, you could."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-ljB8DZ5Aj2",
        "outputId": "ec1fc2cf-eecf-4668-8dc9-8449fdcf923b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7e21297d1c90>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7e2130c81390>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Create training and testing DataLoaders as well as get a list of class names\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=manual_transforms, # resize, convert images to between 0 & 1 and normalize them\n",
        "                                                                               batch_size=32) # set mini-batch size to 32\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlmB-Dlo5Aj3"
      },
      "source": [
        "#### Syntax of Using `.py` Scripts  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxE8E5D95Aj3"
      },
      "source": [
        "\n",
        "- **Importing the Script**:  \n",
        "  Earlier in the notebook, the `data_setup` module was imported using the line:  \n",
        "  ```python\n",
        "  from going_modular import data_setup\n",
        "  ```  \n",
        "  This means that `data_setup` is a Python script (`.py` file) located in the `going_modular` directory, and it contains functions that can be used in the notebook.  \n",
        "\n",
        "- **Calling a Function from the Script**:  \n",
        "  The syntax `data_setup.create_dataloaders` is used to call the `create_dataloaders` function defined in the `data_setup.py` script. This is a common pattern in Python where you import a module and then access its functions using dot notation (`module.function_name`).  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZwOyYdy5Aj3"
      },
      "source": [
        "### 2.2 Creating a transform for `torchvision.models` (auto creation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7j8kF745Aj7"
      },
      "source": [
        "\n",
        "As previously stated, when using a pretrained model, it's important that **your custom data going into the model is prepared in the same way as the original training data that went into the model**.\n",
        "\n",
        "Above we saw how to manually create a transform for a pretrained model.\n",
        "\n",
        "But as of `torchvision` v0.13+, an automatic transform creation feature has been added.\n",
        "\n",
        "When you setup a model from `torchvision.models` and select the pretrained model weights you'd like to use, for example, say we'd like to use:\n",
        "    \n",
        "```python\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "```\n",
        "\n",
        "Where,\n",
        "* `EfficientNet_B0_Weights` is the model architecture weights we'd like to use (there are many different model architecture options in `torchvision.models`).\n",
        "* `DEFAULT` means the *best available* weights (the best performance in ImageNet).\n",
        "    * **Note:** Depending on the model architecture you choose, you may also see other options such as `IMAGENET_V1` and `IMAGENET_V2` where generally the higher version number the better. Though if you want the best available, `DEFAULT` is the easiest option. See the [`torchvision.models` documentation](https://pytorch.org/vision/main/models.html) for more.\n",
        "    \n",
        "Let's try it out."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}