{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JunHL96/PyTorch-Course/blob/main/04_pytorch_custom_datasets_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_lD21iv_Tnc"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1LH4HinVwLGL-gYXot2zkoCIdxtMMI5N_\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "[View Source Code](https://github.com/JunHL96/PyTorch-Course/blob/main/04_pytorch_custom_datasets_notes.ipynb) | [View Slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/04_pytorch_custom_datasets.pdf) | [Watch Video Walkthrough](https://youtu.be/Z_ikDlimN6A?t=71010)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMEuM1bW_Tnd"
      },
      "source": [
        "# 04. PyTorch Custom Datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XQr8h--_Tnd"
      },
      "source": [
        "\n",
        "In the last notebook, [notebook 03](https://www.learnpytorch.io/03_pytorch_computer_vision/), we looked at how to build computer vision models on an in-built dataset in PyTorch (FashionMNIST).\n",
        "\n",
        "The steps we took are similar across many different problems in machine learning.\n",
        "\n",
        "Find a dataset, turn the dataset into numbers, build a model (or find an existing model) to find patterns in those numbers that can be used for prediction.\n",
        "\n",
        "PyTorch has many built-in datasets used for a wide number of machine learning benchmarks, however, you'll often want to use your own **custom dataset**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "109izf6r_Tnd"
      },
      "source": [
        "## What is a custom dataset?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBkm53VO_Tne"
      },
      "source": [
        "\n",
        "A **custom dataset** is a collection of data relating to a specific problem you're working on.\n",
        "\n",
        "In essence, a **custom dataset** can be comprised of almost anything.\n",
        "\n",
        "For example, if we were building a food image classification app like [Nutrify](https://nutrify.app), our custom dataset might be images of food.\n",
        "\n",
        "Or if we were trying to build a model to classify whether or not a text-based review on a website was positive or negative, our custom dataset might be examples of existing customer reviews and their ratings.\n",
        "\n",
        "Or if we were trying to build a sound classification app, our custom dataset might be sound samples alongside their sample labels.\n",
        "\n",
        "Or if we were trying to build a recommendation system for customers purchasing things on our website, our custom dataset might be examples of products other people have bought.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pytorch-domain-libraries.png\" alt=\"different pytorch domain libraries can be used for specific PyTorch problems\" width=1000/>\n",
        "\n",
        "*PyTorch includes many existing functions to load in various custom datasets in the [`TorchVision`](https://pytorch.org/vision/stable/index.html), [`TorchText`](https://pytorch.org/text/stable/index.html), [`TorchAudio`](https://pytorch.org/audio/stable/index.html) and [`TorchRec`](https://pytorch.org/torchrec/) domain libraries.*\n",
        "\n",
        "But sometimes these existing functions may not be enough.\n",
        "\n",
        "In that case, we can always subclass `torch.utils.data.Dataset` and customize it to our liking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "984odNRh_Tne"
      },
      "source": [
        "## What we're going to cover\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0LOTFWz_Tne"
      },
      "source": [
        "\n",
        "We're going to be applying the PyTorch Workflow we covered in [notebook 01](https://www.learnpytorch.io/01_pytorch_workflow/) and [notebook 02](https://www.learnpytorch.io/02_pytorch_classification/) to a computer vision problem.\n",
        "\n",
        "But instead of using an in-built PyTorch dataset, we're going to be using our own dataset of pizza, steak and sushi images.\n",
        "\n",
        "The goal will be to load these images and then build a model to train and predict on them.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pytorch-food-vision-layout.png\" alt=\"building a pipeline to load in food images and then building a pytorch model to classify those food images\" width=800 />\n",
        "\n",
        "*What we're going to build. We'll use `torchvision.datasets` as well as our own custom `Dataset` class to load in images of food and then we'll build a PyTorch computer vision model to hopefully be able to classify them.*\n",
        "\n",
        "Specifically, we're going to cover:\n",
        "\n",
        "| **Topic** | **Contents** |\n",
        "| ----- | ----- |\n",
        "| **0. Importing PyTorch and setting up device-agnostic code** | Let's get PyTorch loaded and then follow best practice to setup our code to be device-agnostic.  |\n",
        "| **1. Get data** | We're going to be using our own **custom dataset** of pizza, steak and sushi images. |\n",
        "| **2. Become one with the data (data preparation)** | At the beginning of any new machine learning problem, it's paramount to understand the data you're working with. Here we'll take some steps to figure out what data we have. |\n",
        "| **3. Transforming data** |Often, the data you get won't be 100% ready to use with a machine learning model, here we'll look at some steps we can take to *transform* our images so they're ready to be used with a model. |\n",
        "| **4. Loading data with `ImageFolder` (option 1)** | PyTorch has many in-built data loading functions for common types of data. `ImageFolder` is helpful if our images are in standard image classification format. |\n",
        "| **5. Loading image data with a custom `Dataset`** | What if PyTorch didn't have an in-built function to load data with? This is where we can build our own custom subclass of `torch.utils.data.Dataset`. |\n",
        "| **6. Other forms of transforms (data augmentation)** | Data augmentation is a common technique for expanding the diversity of your training data. Here we'll explore some of `torchvision`'s in-built data augmentation functions. |\n",
        "| **7. Model 0: TinyVGG without data augmentation** | By this stage, we'll have our data ready, let's build a model capable of fitting it. We'll also create some training and testing functions for training and evaluating our model. |\n",
        "| **8. Exploring loss curves** | Loss curves are a great way to see how your model is training/improving over time. They're also a good way to see if your model is **underfitting** or **overfitting**. |\n",
        "| **9. Model 1: TinyVGG with data augmentation** | By now, we've tried a model *without*, how about we try one *with* data augmentation? |\n",
        "| **10. Compare model results** | Let's compare our different models' loss curves and see which performed better and discuss some options for improving performance. |\n",
        "| **11. Making a prediction on a custom image** | Our model is trained to on a dataset of pizza, steak and sushi images. In this section we'll cover how to use our trained model to predict on an image *outside* of our existing dataset. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQCw5WCb_Tne"
      },
      "source": [
        "## 0. Importing PyTorch and setting up device-agnostic code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XQRQk72_Tne",
        "outputId": "5faf2a50-e0af-4bda-df83-78828d852132"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.5.1'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Note: this notebook requires torch >= 1.10.0\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCyhyEhM_Tnf"
      },
      "source": [
        "And now let's follow best practice and setup device-agnostic code.\n",
        "\n",
        "> **Note:** If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`. If you do this, your runtime will likely reset and you'll have to run all of the cells above by going `Runtime -> Run before`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwDmHoeE_Tnf",
        "outputId": "ab778951-0f18-464e-962f-b273b95e2bd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "PyTorch version: 2.5.1\n"
          ]
        }
      ],
      "source": [
        "# Device-Agnostic Code\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\" # NVIDIA GPU\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\" # Apple GPU\n",
        "else:\n",
        "    device = \"cpu\" # Defaults to CPU if NVIDIA GPU/Apple GPU aren't available\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Check versions\n",
        "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKjmsVSz_Tnf"
      },
      "source": [
        "## 1. Acquire Data\n",
        "\n",
        "To begin, we need a dataset. Fortunately, some data has already been prepared for us.\n",
        "\n",
        "We'll start with a small subset because our goal isn't to train the largest model or use the biggest dataset initially. Machine learning is an iterative process: start small, achieve functionality, then scale up as needed.\n",
        "\n",
        "We will use a subset of the [Food101 dataset](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/). Food101 is a well-known computer vision benchmark containing 101,000 images of 101 different food categories (75,750 for training and 25,250 for testing).\n",
        "\n",
        "Instead of working with all 101 food classes, we'll focus on three: pizza, steak, and sushi. Additionally, we'll use a random 10% of the images per class to keep the dataset manageable.\n",
        "\n",
        "If you're interested in the data sources, refer to the following resources:\n",
        "* [Original Food101 Dataset and Paper](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/)\n",
        "* [`torchvision.datasets.Food101`](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html) - The version used in this notebook.\n",
        "* [`extras/04_custom_data_creation.ipynb`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb) - Notebook used to format the Food101 dataset for this project.\n",
        "* [`data/pizza_steak_sushi.zip`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi.zip) - Zip archive containing the selected images.\n",
        "\n",
        "Next, we'll write code to download the formatted data from GitHub.\n",
        "\n",
        "> **Note:** While the dataset we're using has been pre-formatted for our purposes, you will often need to format your own datasets to suit your specific problems. This is a common practice in the machine learning field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJMdYJ3Q_Tnf",
        "outputId": "7cf35bb7-3220-455b-aa84-f984832aa013"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Did not find data/pizza_steak_sushi directory, creating one...\n",
            "Downloading pizza, steak, sushi data...\n",
            "Unzipping pizza, steak, sushi data...\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download pizza, steak, sushi data\n",
        "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "        print(\"Downloading pizza, steak, sushi data...\")\n",
        "        f.write(request.content)\n",
        "\n",
        "    # Unzip pizza, steak, sushi data\n",
        "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "        print(\"Unzipping pizza, steak, sushi data...\")\n",
        "        zip_ref.extractall(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1Z1tMMN_Tnf"
      },
      "source": [
        "## 2. Understand the Data (Data Preparation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFwF83FL_Tnf"
      },
      "source": [
        "\n",
        "With the dataset downloaded, the next crucial step is to familiarize ourselves with it. This process, often called *data preparation*, is essential before building a machine learning model.\n",
        "\n",
        "As Abraham Lossfunction said...\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-abraham-lossfunction.png\" alt=\"tweet by mrdbourke, if I had eight hours to build a machine learning model, I'd spend the first 6 hours preparing my dataset\" width=800/>\n",
        "\n",
        "Data preparation involves deeply understanding the dataset and its structure. Before diving into model development, it's vital to ask:  **What am I trying to do with this data?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-QyPRpA_Tnf"
      },
      "source": [
        "\n",
        "### Dataset Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7kWwrrx_Tng"
      },
      "source": [
        "\n",
        "Our dataset consists of images of pizza, steak, and sushi, organized in a standard *image classification format*. This format is widely used in image classification benchmarks, including datasets like [ImageNet](https://www.image-net.org/). In this format:\n",
        "- Each image class has its own directory, named after the class (e.g., `pizza/`, `steak/`, `sushi/`).\n",
        "- These directories are further divided into training (`train/`) and testing (`test/`) subsets.\n",
        "\n",
        "\n",
        "This format is popular across many different image classification benchmarks, including [ImageNet](https://www.image-net.org/) (of the most popular computer vision benchmark datasets).\n",
        "\n",
        "You can see an example of the storage format below, the images numbers are arbitrary.\n",
        "\n",
        "```\n",
        "pizza_steak_sushi/ <- overall dataset folder\n",
        "    train/ <- training images\n",
        "        pizza/ <- class name as folder name\n",
        "            image01.jpeg\n",
        "            image02.jpeg\n",
        "            ...\n",
        "        steak/\n",
        "            image24.jpeg\n",
        "            image25.jpeg\n",
        "            ...\n",
        "        sushi/\n",
        "            image37.jpeg\n",
        "            ...\n",
        "    test/ <- testing images\n",
        "        pizza/\n",
        "            image101.jpeg\n",
        "            image102.jpeg\n",
        "            ...\n",
        "        steak/\n",
        "            image154.jpeg\n",
        "            image155.jpeg\n",
        "            ...\n",
        "        sushi/\n",
        "            image167.jpeg\n",
        "            ...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2J2zquF_Tng"
      },
      "source": [
        "\n",
        "### Objective\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzVYHbCo_Tng"
      },
      "source": [
        "\n",
        "The goal is to:\n",
        "1. Analyze the dataset's structure to ensure it meets the requirements for training a machine learning model.\n",
        "2. Convert this structure into a format usable with PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs6Fn9Hz_Tng"
      },
      "source": [
        "\n",
        "### Key Concepts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thjDQNua_Tng"
      },
      "source": [
        "\n",
        "1. **Data Inspection**:\n",
        "   - Carefully review the dataset's contents.\n",
        "   - Ensure the images and their classifications are correctly organized.\n",
        "   - This helps you understand what you're working with and spot any potential issues.\n",
        "\n",
        "2. **Data Storage Format**:\n",
        "   - The dataset follows a hierarchical folder structure with class names as folder names.\n",
        "   - Separate training and testing sets help in evaluating model performance.\n",
        "\n",
        "3. **Data Preparation**:\n",
        "   - Adapt the existing data format to be compatible with the tools you plan to use (e.g., PyTorch).\n",
        "   - Ensure the dataset is cleaned and properly structured for optimal model training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CakoHJVt_Tng"
      },
      "source": [
        "\n",
        "### Why This Matters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnMlkI7n_Tng"
      },
      "source": [
        "\n",
        "The structure and quality of your data directly impact the performance of your machine learning model. A well-prepared dataset minimizes errors, reduces preprocessing overhead, and helps your model learn effectively.\n",
        "\n",
        "> **Note:** Data preparation varies depending on the problem you're solving, but the core principle remains the same: understand your data thoroughly, then find a way to best turn it into a dataset compatible with PyTorch.\n",
        "\n",
        "We can inspect what's in our data directory by writing a small helper function to walk through each of the subdirectories and count the files present.\n",
        "\n",
        "To do so, we'll use Python's in-built [`os.walk()`](https://docs.python.org/3/library/os.html#os.walk)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LynFoEB__Tng",
        "outputId": "2c36ac81-4729-4e5a-8af6-e9d8320b5317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 2 directories and 0 images in 'data/pizza_steak_sushi'.\n",
            "There are 3 directories and 0 images in 'data/pizza_steak_sushi/test'.\n",
            "There are 0 directories and 19 images in 'data/pizza_steak_sushi/test/steak'.\n",
            "There are 0 directories and 31 images in 'data/pizza_steak_sushi/test/sushi'.\n",
            "There are 0 directories and 25 images in 'data/pizza_steak_sushi/test/pizza'.\n",
            "There are 3 directories and 0 images in 'data/pizza_steak_sushi/train'.\n",
            "There are 0 directories and 75 images in 'data/pizza_steak_sushi/train/steak'.\n",
            "There are 0 directories and 72 images in 'data/pizza_steak_sushi/train/sushi'.\n",
            "There are 0 directories and 78 images in 'data/pizza_steak_sushi/train/pizza'.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "def walk_through_dir(dir_path):\n",
        "  \"\"\"\n",
        "  Walks through dir_path returning its contents.\n",
        "  Args:\n",
        "    dir_path (str or pathlib.Path): target directory\n",
        "\n",
        "  Returns:\n",
        "    A print out of:\n",
        "      number of subdiretories in dir_path\n",
        "      number of images (files) in each subdirectory\n",
        "      name of each subdirectory\n",
        "  \"\"\"\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "\n",
        "walk_through_dir(image_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}