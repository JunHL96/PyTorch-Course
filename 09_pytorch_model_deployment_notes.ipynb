{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAFX2QxjaOJW"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/12YBmVsk5KiY4ffV908P4jzIiwSdEzFOV#scrollTo=LSwef9fyVOPj\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "[View Source Code](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/09_pytorch_model_deployment.ipynb) | [View Slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/09_pytorch_model_deployment.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdErrbSdaOJX"
      },
      "source": [
        "# 09. PyTorch Model Deployment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijVxJIw-aOJX"
      },
      "source": [
        "\n",
        "Welcome to Milestone Project 3: PyTorch Model Deployment!\n",
        "\n",
        "We've come a long way with our FoodVision Mini project.\n",
        "\n",
        "But so far our PyTorch models have only been accessible to us.\n",
        "\n",
        "How about we bring FoodVision Mini to life and make it publically accessible?\n",
        "\n",
        "In other words, **we're going to deploy our FoodVision Mini model to the internet as a usable app!**\n",
        "\n",
        "<img src=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/09-model-deployment-what-were-doing-demo-trimmed-cropped-small.gif\" alt=\"demo of foodvision mini computer vision model being used on a mobile device to predict on an image of sushi and getting it right\" width=900/>\n",
        "\n",
        "*Trying out the [deployed version of FoodVision Mini](https://huggingface.co/spaces/mrdbourke/foodvision_mini) (what we're going to build) on my lunch. The model got it right too üç£!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCFy9U6baOJX"
      },
      "source": [
        "## What is machine learning model deployment?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8at2a4laOJX"
      },
      "source": [
        "\n",
        "**Machine learning model deployment** is the process of making your machine learning model accessible to someone or something else.\n",
        "\n",
        "Someone else being a person who can interact with your model in some way.\n",
        "\n",
        "For example, someone taking a photo on their smartphone of food and then having our FoodVision Mini model classify it into pizza, steak or sushi.\n",
        "\n",
        "Something else might be another program, app or even another model that interacts with your machine learning model(s).\n",
        "\n",
        "For example, a banking database might rely on a machine learning model making predictions as to whether a transaction is fraudulent or not before transferring funds.\n",
        "\n",
        "Or an operating system may lower its resource consumption based on a machine learning model making predictions on how much power someone generally uses at specific times of day.\n",
        "\n",
        "These use cases can be mixed and matched as well.\n",
        "\n",
        "For example, a Tesla car's computer vision system will interact with the car's route planning program (something else) and then the route planning program will get inputs and feedback from the driver (someone else).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-what-is-model-deployment-someone-or-something-else.png\" width=900 alt=\"two use cases for model deployment, making your model available to someone else, for example, someone using it in an app, or making it available to something else such as another program or model\"/>\n",
        "\n",
        "*Machine learning model deployment involves making your model available to someone or something else. For example, someone might use your model as part of a food recognition app (such as FoodVision Mini or [Nutrify](https://nutrify.app)). And something else might be another model or program using your model such as a banking system using a machine learning model to detect if a transaction is fraud or not.*\n",
        "          \n",
        "          \n",
        "          "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqdHsa1OaOJY"
      },
      "source": [
        "## Why deploy a machine learning model?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DAH889RaOJY"
      },
      "source": [
        "\n",
        "One of the most important philosophical questions in machine learning is:\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-does-it-exist.jpeg\" alt=\"curious dinosaur often referred to as philosoraptor asking the question if a machine learning model never leaves a notebook, does it exist?\" width=300/>\n",
        "</div>\n",
        "\n",
        "Deploying a model is as important as training one.\n",
        "\n",
        "Because although you can get a pretty good idea of how your model's going to function by evaluting it on a well crafted test set or visualizing its results, you never really know how it's going to perform until you release it to the wild.\n",
        "\n",
        "Having people who've never used your model interact with it will often reveal edge cases you never thought of during training.\n",
        "\n",
        "For example, what happens if someone was to upload a photo that *wasn't* of food to our FoodVision Mini model?\n",
        "\n",
        "One solution would be to create another model that first classifies images as \"food\" or \"not food\" and passing the target image through that model first (this is what [Nutrify](https://nutrify.app) does).\n",
        "\n",
        "Then if the image is of \"food\" it goes to our FoodVision Mini model and gets classified into pizza, steak or sushi.\n",
        "\n",
        "And if it's \"not food\", a message is displayed.\n",
        "\n",
        "But what if these predictions were wrong?\n",
        "\n",
        "What happens then?\n",
        "\n",
        "You can see how these questions could keep going.\n",
        "\n",
        "Thus this highlights the importance of model deployment: it helps you figure out errors in your model that aren't obvious during training/testing.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-pytorch-workflow-with-deployment.png\" alt=\"A PyTorch workflow with added model deployment and monitoring step\" width=900/>\n",
        "\n",
        "*We covered a PyTorch workflow back in [01. PyTorch Workflow](https://www.learnpytorch.io/01_pytorch_workflow/). But once you've got a good model, deployment is a good next step. Monitoring involves seeing how your model goes on the most important data split: data from the real world. For more resources on deployment and monitoring see [PyTorch Extra Resources](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and-deep-learning-engineering).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EN_Dy9FaOJY"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVdXE6UaaOJY"
      },
      "source": [
        "## Different types of machine learning model deployment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m31rYaKfaOJY"
      },
      "source": [
        "\n",
        "Whole books could be written on the different types of machine learning model deployment (and many good ones are listed in [PyTorch Extra Resources](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and-deep-learning-engineering)).\n",
        "\n",
        "And the field is still developing in terms of best practices.\n",
        "\n",
        "But I like to start with the question:\n",
        "\n",
        "> \"What is the most ideal scenario for my machine learning model to be used?\"\n",
        "\n",
        "And then work backwards from there.\n",
        "\n",
        "Of course, you may not know this ahead of time. But you're smart enough to imagine such things.\n",
        "\n",
        "In the case of FoodVision Mini, our ideal scenario might be:\n",
        "\n",
        "* Someone takes a photo on a mobile device (through an app or web broswer).\n",
        "* The prediction comes back fast.\n",
        "\n",
        "Easy.\n",
        "\n",
        "So we've got two main criteria:\n",
        "\n",
        "1. The model should work on a mobile device (this means there will be some compute constraints).\n",
        "2. The model should make predictions *fast* (because a slow app is a boring app).\n",
        "\n",
        "And of course, depending on your use case, your requirements may vary.\n",
        "\n",
        "You may notice the above two points break down into another two questions:\n",
        "\n",
        "1. **Where's it going to go?** - As in, where is it going to be stored?\n",
        "2. **How's it going to function?** - As in, does it return predictions immediately? Or do they come later?\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-deployment-questions-to-ask.png\" alt=\"some questions to ask when starting to deploy machine learning models, what's the model ideal use case, then work backwards and ask where's my model going to go and how's my model going to function\" width=900/>\n",
        "\n",
        "*When starting to deploy machine learning models, it's helpful to start by asking what's the most ideal use case and then work backwards from there, asking where the model's going to go and then how it's going to function.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IahVZPyIaOJY"
      },
      "source": [
        "### Where's it going to go?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0-1VskjaOJY"
      },
      "source": [
        "\n",
        "When you deploy your machine learning model, where does it live?\n",
        "\n",
        "The main debate here is usually on-device (also called edge/in the browser) or on the cloud (a computer/server that isn't the *actual* device someone/something calls the model from).\n",
        "\n",
        "Both have their pros and cons.\n",
        "\n",
        "| **Deployment location** | **Pros** | **Cons** |\n",
        "| ----- | ----- | ----- |\n",
        "| **On-device (edge/in the browser)** | Can be very fast (since no data leaves the device) | Limited compute power (larger models take longer to run) |\n",
        "| | Privacy preserving (again no data has to leave the device) | Limited storage space (smaller model size required) |\n",
        "| | No internet connection required (sometimes) | Device-specific skills often required |\n",
        "| | | |\n",
        "| **On cloud** | Near unlimited compute power (can scale up when needed) | Costs can get out of hand (if proper scaling limits aren't enforced) |\n",
        "| | Can deploy one model and use everywhere (via API) | Predictions can be slower due to data having to leave device and predictions having to come back (network latency) |\n",
        "| | Links into existing cloud ecosystem | Data has to leave device (this may cause privacy concerns) |\n",
        "\n",
        "There are more details to these but I've left resources in the [extra-curriculum](https://www.learnpytorch.io/09_pytorch_model_deployment/#extra-curriculum) to learn more.\n",
        "\n",
        "Let's give an example.\n",
        "\n",
        "If we're deploying FoodVision Mini as an app, we want it to perform well and fast.\n",
        "\n",
        "So which model would we prefer?\n",
        "\n",
        "1. A model on-device that performs at 95% accuracy with an inference time (latency) of one second per prediction.\n",
        "2. A model on the cloud that performs at 98% accuracy with an inference time of 10 seconds per per prediction (bigger, better model but takes longer to compute).\n",
        "\n",
        "I've made these numbers up but they showcase a potential difference between on-device and on the cloud.\n",
        "\n",
        "Option 1 could potentially be a smaller less performant model that runs fast because its able to fit on a mobile device.\n",
        "\n",
        "Option 2 could potentially a larger more performant model that requires more compute and storage but it takes a bit longer to run because we have to send data off the device and get it back (so even though the actual prediction might be fast, the network time and data transfer has to factored in).\n",
        "\n",
        "For FoodVision Mini, we'd likely prefer option 1, because the small hit in performance is far outweighed by the faster inference speed.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-on-device-vs-cloud.png\" width=900 alt=\"tesla computer vision system on device vs on the cloud\"/>\n",
        "\n",
        "*In the case of a Tesla car's computer vision system, which would be better? A smaller model that performs well on device (model is on the car) or a larger model that performs better that's on the cloud? In this case, you'd much prefer the model being on the car. The extra network time it would take for data to go from the car to the cloud and then back to the car just wouldn't be worth it (or potentially even impossible with poor signal areas).*\n",
        "\n",
        "> **Note:** For a full example of seeing what it's like to deploy a PyTorch model to an edge device, see the [PyTorch tutorial on achieving real-time inference (30fps+)](https://pytorch.org/tutorials/intermediate/realtime_rpi.html) with a computer vision model on a Raspberry Pi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IA4jDxAaOJY"
      },
      "source": [
        "### How's it going to function?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kam1gFGkaOJY"
      },
      "source": [
        "\n",
        "Back to the ideal use case, when you deploy your machine learning model, how should it work?\n",
        "\n",
        "As in, would you like predictions returned immediately?\n",
        "\n",
        "Or is it okay for them to happen later?\n",
        "\n",
        "These two scenarios are generally referred to as:\n",
        "\n",
        "* **Online (real-time)** - Predictions/inference happen **immediately**. For example, someone uploads an image, the image gets transformed and predictions are returned or someone makes a purchase and the transaction is verified to be non-fraudulent by a model so the purchase can go through.\n",
        "* **Offline (batch)** - Predictions/inference happen **periodically**. For example, a photos application sorts your images into different categories (such as beach, mealtime, family, friends) whilst your mobile device is plugged into charge.\n",
        "\n",
        "> **Note:** \"Batch\" refers to inference being performed on multiple samples at a time. However, to add a little confusion, batch processing can happen immediately/online (multiple images being classified at once) and/or offline (multiple images being predicted/trained on at once).  \n",
        "\n",
        "The main difference between each being: predictions being made immediately or periodically.\n",
        "\n",
        "Periodically can have a varying timescale too, from every few seconds to every few hours or days.\n",
        "\n",
        "And you can mix and match the two.\n",
        "\n",
        "In the case of FoodVision Mini, we'd want our inference pipeline to happen online (real-time), so when someone uploads an image of pizza, steak or sushi, the prediction results are returned immediately (any slower than real-time would make a boring experience).\n",
        "\n",
        "But for our training pipeline, it's okay for it to happen in a batch (offline) fashion, which is what we've been doing throughout the previous chapters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjZtAx-1aOJY"
      },
      "source": [
        "### Ways to deploy a machine learning model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfE3onhnaOJY"
      },
      "source": [
        "\n",
        "We've discussed a couple of options for deploying machine learning models (on-device and cloud).\n",
        "\n",
        "And each of these will have their specific requirements:\n",
        "\n",
        "| **Tool/resource** | **Deployment type** |\n",
        "| ----- | ----- |\n",
        "| [Google's ML Kit](https://developers.google.com/ml-kit) | On-device (Android and iOS) |\n",
        "| [Apple's Core ML](https://developer.apple.com/documentation/coreml) and [`coremltools` Python package](https://coremltools.readme.io/docs) | On-device (all Apple devices) |\n",
        "| [Amazon Web Service's (AWS) Sagemaker](https://aws.amazon.com/sagemaker/) | Cloud |\n",
        "| [Google Cloud's Vertex AI](https://cloud.google.com/vertex-ai) | Cloud |\n",
        "| [Microsoft's Azure Machine Learning](https://azure.microsoft.com/en-au/services/machine-learning/) | Cloud |\n",
        "| [Hugging Face Spaces](https://huggingface.co/spaces) | Cloud |\n",
        "| API with [FastAPI](https://fastapi.tiangolo.com) | Cloud/self-hosted server |\n",
        "| API with [TorchServe](https://pytorch.org/serve/) | Cloud/self-hosted server |\n",
        "| [ONNX (Open Neural Network Exchange)](https://onnx.ai/index.html) | Many/general |\n",
        "| Many more... ||\n",
        "\n",
        "> **Note:** An [application programming interface (API)](https://en.wikipedia.org/wiki/API) is a way for two (or more) computer programs to interact with each other. For example, if your model was deployed as API, you would be able to write a program that could send data to it and then receive predictions back.\n",
        "\n",
        "Which option you choose will be highly dependent on what you're building/who you're working with.\n",
        "\n",
        "But with so many options, it can be very intimidating.\n",
        "\n",
        "So best to start small and keep it simple.\n",
        "\n",
        "And one of the best ways to do so is by turning your machine learning model into a demo app with [Gradio](https://gradio.app) and then deploying it on Hugging Face Spaces.\n",
        "\n",
        "We'll be doing just that with FoodVision Mini later on.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-tools-and-places-to-deploy-ml-models.png\" alt=\"tools and places to deploy machine learning models\" width=900/>\n",
        "\n",
        "*A handful of places and tools to host and deploy machine learning models. There are plenty I've missed so if you'd like to add more, please leave a [discussion on GitHub](https://github.com/mrdbourke/pytorch-deep-learning/discussions).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB5LNHDuaOJZ"
      },
      "source": [
        "## What we're going to cover\n",
        "\n",
        "Enough talking about deploying a machine learning model.\n",
        "\n",
        "Let's become machine learning engineers and actually deploy one.\n",
        "\n",
        "Our goal is to deploy our FoodVision Model via a demo Gradio app with the following metrics:\n",
        "1. **Performance:** 95%+ accuracy.\n",
        "2. **Speed:** real-time inference of 30FPS+ (each prediction has a latency of lower than ~0.03s).\n",
        "\n",
        "We'll start by running an experiment to compare our best two models so far: EffNetB2 and ViT feature extractors.\n",
        "\n",
        "Then we'll deploy the one which performs closest to our goal metrics.\n",
        "\n",
        "Finally, we'll finish with a (BIG) surprise bonus.\n",
        "\n",
        "| **Topic** | **Contents** |\n",
        "| ----- | ----- |\n",
        "| **0. Getting setup** | We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. |\n",
        "| **1. Get data** | Let's download the [`pizza_steak_sushi_20_percent.zip`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi_20_percent.zip) dataset so we can train our previously best performing models on the same dataset. |\n",
        "| **2. FoodVision Mini model deployment experiment outline** | Even on the third milestone project, we're still going to be running multiple experiments to see which model (EffNetB2 or ViT) achieves closest to our goal metrics. |\n",
        "| **3. Creating an EffNetB2 feature extractor** | An EfficientNetB2 feature extractor performed the best on our pizza, steak, sushi dataset in [07. PyTorch Experiment Tracking](https://www.learnpytorch.io/07_pytorch_experiment_tracking/), let's recreate it as a candidate for deployment. |\n",
        "| **4. Creating a ViT feature extractor** | A ViT feature extractor has been the best performing model yet on our pizza, steak, sushi dataset in [08. PyTorch Paper Replicating](https://www.learnpytorch.io/08_pytorch_paper_replicating/), let's recreate it as a candidate for deployment alongside EffNetB2. |\n",
        "| **5. Making predictions with our trained models and timing them** | We've built two of the best performing models yet, let's make predictions with them and track their results. |\n",
        "| **6. Comparing model results, prediction times and size** | Let's compare our models to see which performs best with our goals. |\n",
        "| **7. Bringing FoodVision Mini to life by creating a Gradio demo** | One of our models performs better than the other (in terms of our goals), so let's turn it into a working app demo! |\n",
        "| **8. Turning our FoodVision Mini Gradio demo into a deployable app** | Our Gradio app demo works locally, let's prepare it for deployment! |\n",
        "| **9. Deploying our Gradio demo to HuggingFace Spaces** | Let's take FoodVision Mini to the web and make it pubically accessible for all! |\n",
        "| **10. Creating a BIG surprise** | We've built FoodVision Mini, time to step things up a notch. |\n",
        "| **11. Deploying our BIG surprise** | Deploying one app was fun, how about we make it two? |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWSujUucaOJZ"
      },
      "source": [
        "## 0. Getting setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa7Rzj-5aOJZ"
      },
      "source": [
        "\n",
        "As we've done previously, let's make sure we've got all of the modules we'll need for this section.\n",
        "\n",
        "We'll import the Python scripts (such as `data_setup.py` and `engine.py`) we created in [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/).\n",
        "\n",
        "To do so, we'll download [`going_modular`](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/going_modular) directory from the [`pytorch-deep-learning` repository](https://github.com/mrdbourke/pytorch-deep-learning) (if we don't already have it).\n",
        "\n",
        "We'll also get the [`torchinfo`](https://github.com/TylerYep/torchinfo) package if it's not available.\n",
        "\n",
        "`torchinfo` will help later on to give us a visual representation of our model.\n",
        "\n",
        "And since later on we'll be using `torchvision` v0.13 package (available as of July 2022), we'll make sure we've got the latest versions.\n",
        "\n",
        "> **Note:** If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8GYTRijaOJZ",
        "outputId": "4f42c2d7-4408-40dd-8d12-55d6a96cf672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.6.0+cu124\n",
            "torchvision version: 0.21.0+cu124\n"
          ]
        }
      ],
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"torchvision version: {torchvision.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jD6TxraaOJZ",
        "outputId": "0ead0a31-4d85-491c-ddee-ab7308127a4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 4393, done.\u001b[K\n",
            "remote: Counting objects: 100% (1534/1534), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 4393 (delta 1458), reused 1399 (delta 1399), pack-reused 2859 (from 2)\u001b[K\n",
            "Receiving objects: 100% (4393/4393), 650.71 MiB | 28.66 MiB/s, done.\n",
            "Resolving deltas: 100% (2660/2660), done.\n",
            "Updating files: 100% (248/248), done.\n"
          ]
        }
      ],
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyUakorYaOJZ",
        "outputId": "0376dc59-8422-430e-cda5-7bd3f1c0fcc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def get_device():\n",
        "    # Check if CUDA is available\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(\"Using CUDA\")\n",
        "    # Check if MPS (Metal Performance Shaders) is available (for Apple Silicon)\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        print(\"Using MPS\")\n",
        "    # Default to CPU if neither CUDA nor MPS is available\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"Using CPU\")\n",
        "    return device\n",
        "\n",
        "# Set the device\n",
        "device = get_device()\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTOvVP5NaOJZ"
      },
      "source": [
        "## 1. Getting data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jl_4QAGaOJZ"
      },
      "source": [
        "\n",
        "We left off in [08. PyTorch Paper Replicating](https://www.learnpytorch.io/08_pytorch_paper_replicating/#106-save-feature-extractor-vit-model-and-check-file-size) comparing our own Vision Transformer (ViT) feature extractor model to the EfficientNetB2 (EffNetB2) feature extractor model we created in [07. PyTorch Experiment Tracking](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#9-load-in-the-best-model-and-make-predictions-with-it).\n",
        "\n",
        "And we found that there was a slight difference in the comparison.\n",
        "\n",
        "The EffNetB2 model was trained on 20% of the pizza, steak and sushi data from Food101 where as the ViT model was trained on 10%.\n",
        "\n",
        "Since our goal is to deploy the best model for our FoodVision Mini problem, let's start by downloading the [20% pizza, steak and sushi dataset](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi_20_percent.zip) and train an EffNetB2 feature extractor and ViT feature extractor on it and then compare the two models.\n",
        "\n",
        "This way we'll be comparing apples to apples (one model trained on a dataset to another model trained on the same dataset).\n",
        "\n",
        "> **Note:** The dataset we're downloading is a sample of the entire [Food101 dataset](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html#food101) (101 food classes with 1,000 images each). More specifically, 20% refers to 20% of images from the pizza, steak and sushi classes selected at random. You can see how this dataset was created in [`extras/04_custom_data_creation.ipynb`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb) and more details in [04. PyTorch Custom Datasets section 1](https://www.learnpytorch.io/04_pytorch_custom_datasets/#1-get-data).\n",
        "\n",
        "We can download the data using the `download_data()` function we created in [07. PyTorch Experiment Tracking section 1](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data) from [`helper_functions.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDyCUbRraOJa",
        "outputId": "adc76016-81fc-44ea-daac-188ae84828ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Did not find data/pizza_steak_sushi_20_percent directory, creating one...\n",
            "[INFO] Downloading pizza_steak_sushi_20_percent.zip from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip...\n",
            "[INFO] Unzipping pizza_steak_sushi_20_percent.zip data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('data/pizza_steak_sushi_20_percent')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Download pizza, steak, sushi images from GitHub\n",
        "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
        "                                     destination=\"pizza_steak_sushi_20_percent\")\n",
        "\n",
        "data_20_percent_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jirkaY79aOJa"
      },
      "source": [
        "Wonderful!\n",
        "\n",
        "Now we've got a dataset, let's create training and test paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nn8HZhlTaOJa"
      },
      "outputs": [],
      "source": [
        "# Setup directory paths to train and test images\n",
        "train_dir = data_20_percent_path / \"train\"\n",
        "test_dir = data_20_percent_path / \"test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TYRp89CaOJa"
      },
      "source": [
        "## 2. FoodVision Mini model deployment experiment outline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYItI4sAaOJa"
      },
      "source": [
        "\n",
        "The ideal deployed model FoodVision Mini performs well and fast.\n",
        "\n",
        "We'd like our model to perform as close to real-time as possible.\n",
        "\n",
        "Real-time in this case being ~30FPS (frames per second) because that's [about how fast the human eye can see](https://www.healthline.com/health/human-eye-fps) (there is debate on this but let's just use ~30FPS as our benchmark).\n",
        "\n",
        "And for classifying three different classes (pizza, steak and sushi), we'd like a model that performs at 95%+ accuracy.\n",
        "\n",
        "Of course, higher accuracy would be nice but this might sacrifice speed.\n",
        "\n",
        "So our goals are:\n",
        "\n",
        "1. **Performance** - A model that performs at 95%+ accuracy.\n",
        "2. **Speed** - A model that can classify an image at ~30FPS (0.03 seconds inference time per image, also known as latency).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployments-speed-vs-inference.png\" alt=\"foodvision mini goals in terms of performance and inference time.\" width=750/>\n",
        "\n",
        "*FoodVision Mini deployment goals. We'd like a fast predicting well-performing model (because a slow app is boring).*\n",
        "\n",
        "We'll put an emphasis on speed, meaning, we'd prefer a model performing at 90%+ accuracy at ~30FPS than a model performing 95%+ accuracy at 10FPS.\n",
        "\n",
        "To try and achieve these results, let's bring in our best performing models from the previous sections:\n",
        "\n",
        "1. **EffNetB2 feature extractor** (EffNetB2 for short) - originally created in [07. PyTorch Experiment Tracking section 7.5](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#75-create-feature-extractor-models) using [`torchvision.models.efficientnet_b2()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#efficientnet-b2) with adjusted `classifier` layers.\n",
        "2. **ViT-B/16 feature extractor** (ViT for short) - originally created in [08. PyTorch Paper Replicating section 10](https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-using-a-pretrained-vit-from-torchvisionmodels-on-the-same-dataset) using [`torchvision.models.vit_b_16()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#vit-b-16) with adjusted `head` layers.\n",
        "    * **Note** ViT-B/16 stands for \"Vision Transformer Base, patch size 16\".\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-two-experiments.png\" alt=\"modelling experiments for foodvision mini deployments, one effnetb2 feature extractor model and a vision transformer feature extractor model\" width=750 />\n",
        "\n",
        "> **Note:** A \"feature extractor model\" often starts with a model that has been pretrained on a dataset similar to your own problem. The pretrained model's base layers are often left frozen (the pretrained patterns/weights stay the same) whilst some of the top (or classifier/classification head) layers get customized to your own problem by training on your own data. We covered the concept of a feature extractor model in [06. PyTorch Transfer Learning section 3.4](https://www.learnpytorch.io/06_pytorch_transfer_learning/#34-freezing-the-base-model-and-changing-the-output-layer-to-suit-our-needs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7mQt7z5aOJa"
      },
      "source": [
        "## 3. Creating an EffNetB2 feature extractor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcc34q_faOJa"
      },
      "source": [
        "\n",
        "We first created an EffNetB2 feature extractor model in [07. PyTorch Experiment Tracking section 7.5](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#75-create-feature-extractor-models).\n",
        "\n",
        "And by the end of that section we saw it performed very well.\n",
        "\n",
        "So let's now recreate it here so we can compare its results to a ViT feature extractor trained on the same data.\n",
        "\n",
        "> **Note:** A feature extractor is a model where most of the pretrained layers are frozen (their weights are kept unchanged), and only the final classification layers are modified and trained on new data. This leverages the pretrained model's ability to extract meaningful features while adapting it to a new task.\n",
        "\n",
        "\n",
        "To do so we can:\n",
        "1. Setup the pretrained weights as [`weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT`](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.EfficientNet_B2_Weights), where \"`DEFAULT`\" means \"best currently available\" (or could use `weights=\"DEFAULT\"`).\n",
        "2. Get the pretrained model image transforms from the weights with the `transforms()` method (we need these so we can convert our images into the same format as the pretrained EffNetB2 was trained on).\n",
        "3. Create a pretrained model instance by passing the weights to an instance of [`torchvision.models.efficientnet_b2`](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#efficientnet-b2).\n",
        "4. Freeze the base layers in the model.\n",
        "5. Update the classifier head to suit our own data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0aPV4rtaOJa",
        "outputId": "48542781-64df-4657-b850-3c8fc4469eeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35.2M/35.2M [00:00<00:00, 95.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# 1. Setup pretrained EffNetB2 weights\n",
        "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "\n",
        "# 2. Get EffNetB2 transforms\n",
        "effnetb2_transforms = effnetb2_weights.transforms()\n",
        "\n",
        "# 3. Setup pretrained model\n",
        "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n",
        "\n",
        "# 4. Freeze the base layers in the model (this will freeze all layers to begin with)\n",
        "for param in effnetb2.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjzSB7unaOJa"
      },
      "source": [
        "- **1. Setup pretrained EffNetB2 weights**\n",
        "  - `effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT`\n",
        "    - This line retrieves the default pretrained weights for the EfficientNet-B2 model. These weights are the result of training the model on a large dataset like ImageNet, allowing the model to perform well on tasks like image classification without needing to be trained from scratch.\n",
        "    - **B2_weights**: These are the pretrained weights specifically for the EfficientNet-B2 architecture. They contain the learned parameters (e.g., convolutional filters, biases) that enable the model to make accurate predictions.\n",
        "\n",
        "- **2. Get EffNetB2 transforms**\n",
        "  - `effnetb2_transforms = effnetb2_weights.transforms()`\n",
        "    - This line retrieves the data preprocessing transforms associated with the pretrained EfficientNet-B2 model. These transforms are necessary to preprocess input images in a way that matches the preprocessing used during the model's training.\n",
        "    - **What does `.transforms()` do?**: It returns a set of transformations (e.g., resizing, normalization) that should be applied to input images before feeding them into the model. These transforms ensure that the input data is in the correct format and scale expected by the pretrained model.\n",
        "\n",
        "- **3. Setup pretrained model**\n",
        "  - `effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)`\n",
        "    - This line initializes the EfficientNet-B2 model with the pretrained weights retrieved earlier. The model is loaded with all its layers and parameters set to the values learned during training on ImageNet.\n",
        "    - The `weights=effnetb2_weights` argument ensures that the model uses the pretrained weights, which saves time and computational resources compared to training from scratch.\n",
        "\n",
        "- **4. Freeze the base layers in the model**\n",
        "  - `for param in effnetb2.parameters(): param.requires_grad = False`\n",
        "    - This loop iterates over all the parameters (e.g., weights and biases) of the EfficientNet-B2 model and sets `requires_grad` to `False`. This effectively \"freezes\" the layers, meaning that their parameters will not be updated during training.\n",
        "    - **Why freeze layers?**: Freezing the base layers is common in transfer learning. It allows you to use the pretrained model as a feature extractor without modifying its learned features. Only the additional layers (e.g., a new classification head) added later will be trained, which is useful when adapting the model to a new task with a smaller dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTatDFhtaOJa"
      },
      "source": [
        "Now to change the classifier head, let's first inspect it using the `classifier` attribute of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kw-xxSwBaOJa",
        "outputId": "b766e14c-87bf-425c-b101-05b46995fd50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Dropout(p=0.3, inplace=True)\n",
              "  (1): Linear(in_features=1408, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Check out EffNetB2 classifier head\n",
        "effnetb2.classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAZeTMG8aOJa"
      },
      "source": [
        "\n",
        "\n",
        "- **Classifier Head**:\n",
        "  - In a convolutional neural network (CNN) like EfficientNet-B2, the **classifier head** is the final part of the model responsible for making predictions based on the features extracted by the earlier layers of the network.\n",
        "  - It typically consists of one or more fully connected (dense) layers that map the high-level features learned by the convolutional layers to the output classes (e.g., the number of classes in a classification task).\n",
        "  - For EfficientNet-B2, the classifier head is a single fully connected layer (`nn.Linear`) that takes the flattened feature vector from the preceding layers and outputs a vector of logits (raw predictions) corresponding to the number of classes in the dataset.\n",
        "\n",
        "- **Structure of EfficientNet-B2 Classifier Head**:\n",
        "  - The classifier head in EfficientNet-B2 is defined as:\n",
        "    ```python\n",
        "    classifier = nn.Sequential(\n",
        "        nn.Dropout(p=dropout_rate, inplace=True),\n",
        "        nn.Linear(in_features, num_classes)\n",
        "    )\n",
        "    ```\n",
        "    - `nn.Dropout`: A dropout layer is applied to prevent overfitting by randomly setting a fraction of input units to 0 during training.\n",
        "    - `nn.Linear`: A fully connected layer that maps the input features (`in_features`) to the output classes (`num_classes`).\n",
        "\n",
        "- **Example Output**:\n",
        "  - Inspecting `effnetb2.classifier`, we see this output:\n",
        "    ```python\n",
        "    Sequential(\n",
        "      (0): Dropout(p=0.3, inplace=True)\n",
        "      (1): Linear(in_features=1408, out_features=1000, bias=True)\n",
        "    )\n",
        "    ```\n",
        "    - Here, `in_features=1408` represents the size of the feature vector output by the preceding layers.\n",
        "    - `out_features=1000` corresponds to the number of classes in the ImageNet dataset (the dataset on which the model was pretrained).\n",
        "\n",
        "- **Purpose**:\n",
        "  - The classifier head is crucial for adapting the model to a specific task. In transfer learning, you often replace or modify this head to match the number of classes in your new dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx9syT9PaOJb"
      },
      "source": [
        "Excellent! To change the classifier head to suit our own problem, let's replace the `out_features` variable with the same number of classes we have (in our case, `out_features=3`, one for pizza, steak, sushi).\n",
        "\n",
        "> **Note:** This process of changing the output layers/classifier head will be dependent on the problem you're working on. For example, if you wanted a different *number* of outputs or a different *kind* of output, you would have to change the output layers accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FjhJtbHSaOJb"
      },
      "outputs": [],
      "source": [
        "# 5. Update the classifier head\n",
        "effnetb2.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.3, inplace=True), # keep dropout layer same\n",
        "    nn.Linear(in_features=1408, # keep in_features same\n",
        "              out_features=3)) # change out_features to suit our number of classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrQnkyILaOJb"
      },
      "source": [
        "### 3.1 Creating a function to make an EffNetB2 feature extractor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHsGnwfMaOJb"
      },
      "source": [
        "\n",
        "Looks like our EffNetB2 feature extractor is ready to go, however, since there's quite a few steps involved here, how about we turn the code above into a function we can re-use later?\n",
        "\n",
        "We'll call it `create_effnetb2_model()` and it'll take a customizable number of classes and a random seed parameter for reproducibility.\n",
        "\n",
        "Ideally, it will return an EffNetB2 feature extractor along with its associated transforms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YA5bTe_taOJb"
      },
      "outputs": [],
      "source": [
        "def create_effnetb2_model(num_classes:int=3,\n",
        "                          seed:int=42):\n",
        "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int, optional): number of classes in the classifier head.\n",
        "            Defaults to 3.\n",
        "        seed (int, optional): random seed value. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): EffNetB2 feature extractor model.\n",
        "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
        "    \"\"\"\n",
        "    # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "    # 4. Freeze all layers in base model\n",
        "    for param in model.parameters():    # param is a variable to iterate through all parameters of the model\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 5. Change classifier head with random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1408, out_features=num_classes),\n",
        "    )\n",
        "\n",
        "    return model, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKB1Kau3aOJc"
      },
      "source": [
        "Woohoo! That's a nice looking function, let's try it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Fyf8hv5CaOJc"
      },
      "outputs": [],
      "source": [
        "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n",
        "                                                      seed=42)\n",
        "\n",
        "# returns a tuple of (model, transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLYtLLNXaOJc"
      },
      "source": [
        "No errors, nice, now to really try it out, let's get a summary with `torchinfo.summary()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu_VflodaOJc",
        "outputId": "864226fd-712f-45f2-f22d-c1d59304ceb8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]     [1, 3]               --                   Partial\n",
              "‚îú‚îÄSequential (features)                                      [1, 3, 224, 224]     [1, 1408, 7, 7]      --                   False\n",
              "‚îÇ    ‚îî‚îÄConv2dNormActivation (0)                              [1, 3, 224, 224]     [1, 32, 112, 112]    --                   False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d (0)                                       [1, 3, 224, 224]     [1, 32, 112, 112]    (864)                False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d (1)                                  [1, 32, 112, 112]    [1, 32, 112, 112]    (64)                 False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄSiLU (2)                                         [1, 32, 112, 112]    [1, 32, 112, 112]    --                   --\n",
              "‚îÇ    ‚îî‚îÄSequential (1)                                        [1, 32, 112, 112]    [1, 16, 112, 112]    --                   False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 32, 112, 112]    [1, 16, 112, 112]    (1,448)              False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 16, 112, 112]    [1, 16, 112, 112]    (612)                False\n",
              "‚îÇ    ‚îî‚îÄSequential (2)                                        [1, 16, 112, 112]    [1, 24, 56, 56]      --                   False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 16, 112, 112]    [1, 24, 56, 56]      (6,004)              False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (2)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "‚îÇ    ‚îî‚îÄSequential (3)                                        [1, 24, 56, 56]      [1, 48, 28, 28]      --                   False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 24, 56, 56]      [1, 48, 28, 28]      (16,518)             False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (2)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "‚îÇ    ‚îî‚îÄSequential (4)                                        [1, 48, 28, 28]      [1, 88, 14, 14]      --                   False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 48, 28, 28]      [1, 88, 14, 14]      (50,300)             False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (2)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (3)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "‚îÇ    ‚îî‚îÄSequential (5)                                        [1, 88, 14, 14]      [1, 120, 14, 14]     --                   False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 88, 14, 14]      [1, 120, 14, 14]     (149,158)            False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (2)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (3)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "‚îÇ    ‚îî‚îÄSequential (6)                                        [1, 120, 14, 14]     [1, 208, 7, 7]       --                   False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 120, 14, 14]     [1, 208, 7, 7]       (301,406)            False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (2)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (3)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (4)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "‚îÇ    ‚îî‚îÄSequential (7)                                        [1, 208, 7, 7]       [1, 352, 7, 7]       --                   False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (0)                                       [1, 208, 7, 7]       [1, 352, 7, 7]       (846,900)            False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄMBConv (1)                                       [1, 352, 7, 7]       [1, 352, 7, 7]       (1,888,920)          False\n",
              "‚îÇ    ‚îî‚îÄConv2dNormActivation (8)                              [1, 352, 7, 7]       [1, 1408, 7, 7]      --                   False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d (0)                                       [1, 352, 7, 7]       [1, 1408, 7, 7]      (495,616)            False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d (1)                                  [1, 1408, 7, 7]      [1, 1408, 7, 7]      (2,816)              False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄSiLU (2)                                         [1, 1408, 7, 7]      [1, 1408, 7, 7]      --                   --\n",
              "‚îú‚îÄAdaptiveAvgPool2d (avgpool)                                [1, 1408, 7, 7]      [1, 1408, 1, 1]      --                   --\n",
              "‚îú‚îÄSequential (classifier)                                    [1, 1408]            [1, 3]               --                   True\n",
              "‚îÇ    ‚îî‚îÄDropout (0)                                           [1, 1408]            [1, 1408]            --                   --\n",
              "‚îÇ    ‚îî‚îÄLinear (1)                                            [1, 1408]            [1, 3]               4,227                True\n",
              "============================================================================================================================================\n",
              "Total params: 7,705,221\n",
              "Trainable params: 4,227\n",
              "Non-trainable params: 7,700,994\n",
              "Total mult-adds (Units.MEGABYTES): 657.64\n",
              "============================================================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 156.80\n",
              "Params size (MB): 30.82\n",
              "Estimated Total Size (MB): 188.22\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Print EffNetB2 model summary (uncomment for full output)\n",
        "summary(effnetb2,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipS1OL9naOJc"
      },
      "source": [
        "Base layers frozen, top layers trainable and customized!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7nxoQDjaOJc"
      },
      "source": [
        "### 3.2 Creating DataLoaders for EffNetB2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqidhZM3aOJc"
      },
      "source": [
        "\n",
        "Our EffNetB2 feature extractor is ready, time to create some `DataLoader`s.\n",
        "\n",
        "We can do this by using the [`data_setup.create_dataloaders()`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/data_setup.py) function we created in [05. PyTorch Going Modular section 2](https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy).\n",
        "\n",
        "We'll use a `batch_size` of 32 and transform our images using the `effnetb2_transforms` so they're in the same format that our `effnetb2` model was trained on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AUKV1e28aOJc"
      },
      "outputs": [],
      "source": [
        "# Setup DataLoaders\n",
        "from going_modular.going_modular import data_setup\n",
        "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                                 test_dir=test_dir,\n",
        "                                                                                                 transform=effnetb2_transforms,\n",
        "                                                                                                 batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0hXEK_EaOJc",
        "outputId": "f90822cd-7dc3-4d19-f99c-224abeb3ccd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pizza', 'steak', 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTA_xU6TaOJc"
      },
      "source": [
        "### 3.3 Training EffNetB2 feature extractor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXN4pqW9aOJc"
      },
      "source": [
        "\n",
        "Model ready, `DataLoader`s ready, let's train!\n",
        "\n",
        "Just like in [07. PyTorch Experiment Tracking section 7.6](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#76-create-experiments-and-set-up-training-code), ten epochs should be enough to get good results.\n",
        "\n",
        "We can do so by creating an optimizer (we'll use [`torch.optim.Adam()`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam) with a learning rate of `1e-3`), a loss function (we'll use [`torch.nn.CrossEntropyLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) for multi-class classification) and then passing these as well as our `DataLoader`s to the [`engine.train()`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py) function we created in [05. PyTorch Going Modular section 4](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228,
          "referenced_widgets": [
            "88b7433c7dd2483f83048aaccf7399cc",
            "41a7921a7c7e4400abc1fd006005ff0a",
            "4a714878c8a84d968cc72402ebffafd9",
            "04b6054cd0d5462ea2cd72a83d0aa011",
            "2b9a4fdbc05f4a06a9506b289fe003ac",
            "9ef0bba27433481ab4e3d9abba6dbcef",
            "1163d11a4a484e8fba1f12422caa9bc2",
            "3784e0fad1a748bc9eb00a80f5a807c3",
            "a5a2470875ca4dd194492ca79240a23a",
            "db903a61101441c382b78132f799e622",
            "f315054b14fc4968b73d46d030119d38"
          ]
        },
        "id": "c3OozBvOaOJc",
        "outputId": "0890d437-5257-4925-c1ae-6fe5a34bda53"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88b7433c7dd2483f83048aaccf7399cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.9839 | train_acc: 0.5667 | test_loss: 0.7393 | test_acc: 0.9409\n",
            "Epoch: 2 | train_loss: 0.7135 | train_acc: 0.8396 | test_loss: 0.5862 | test_acc: 0.9409\n",
            "Epoch: 3 | train_loss: 0.5874 | train_acc: 0.8958 | test_loss: 0.4891 | test_acc: 0.9563\n",
            "Epoch: 4 | train_loss: 0.4488 | train_acc: 0.9146 | test_loss: 0.4338 | test_acc: 0.9409\n",
            "Epoch: 5 | train_loss: 0.4277 | train_acc: 0.9125 | test_loss: 0.3907 | test_acc: 0.9443\n",
            "Epoch: 6 | train_loss: 0.4392 | train_acc: 0.8896 | test_loss: 0.3525 | test_acc: 0.9688\n",
            "Epoch: 7 | train_loss: 0.4246 | train_acc: 0.8771 | test_loss: 0.3263 | test_acc: 0.9563\n",
            "Epoch: 8 | train_loss: 0.3885 | train_acc: 0.8979 | test_loss: 0.3465 | test_acc: 0.9443\n",
            "Epoch: 9 | train_loss: 0.3795 | train_acc: 0.8812 | test_loss: 0.3127 | test_acc: 0.9193\n",
            "Epoch: 10 | train_loss: 0.3752 | train_acc: 0.8688 | test_loss: 0.2811 | test_acc: 0.9625\n"
          ]
        }
      ],
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
        "                             lr=1e-3)\n",
        "# Setup loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Set seeds for reproducibility and train the model\n",
        "set_seeds()\n",
        "effnetb2_results = engine.train(model=effnetb2,\n",
        "                                train_dataloader=train_dataloader_effnetb2,\n",
        "                                test_dataloader=test_dataloader_effnetb2,\n",
        "                                epochs=10,\n",
        "                                optimizer=optimizer,\n",
        "                                loss_fn=loss_fn,\n",
        "                                device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drbE6bs0aOJd"
      },
      "source": [
        "### 3.4 Inspecting EffNetB2 loss curves\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MRWgRmUaOJd"
      },
      "source": [
        "\n",
        "Nice!\n",
        "\n",
        "As we saw in 07. PyTorch Experiment Tracking, the EffNetB2 feature extractor model works quite well on our data.\n",
        "\n",
        "Let's turn its results into loss curves to inspect them further.\n",
        "\n",
        "> **Note:** Loss curves are one of the best ways to visualize how your model's performing. For more on loss curves, check out [04. PyTorch Custom Datasets section 8: What should an ideal loss curve look like?](https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "Z2-IUF1NaOJd",
        "outputId": "1166fc32-27ba-4b6e-f261-1707c99595a0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(effnetb2_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUCuwYAsaOJd"
      },
      "source": [
        "Woah!\n",
        "\n",
        "Those are some nice looking loss curves.\n",
        "\n",
        "It looks like our model is performing quite well and perhaps would benefit from a little longer training and potentially some [data augmentation](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation) (to help prevent potential overfitting occurring from longer training)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2P3ZGHvaOJd"
      },
      "source": [
        "### 3.5 Saving EffNetB2 feature extractor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4dpakwPaOJd"
      },
      "source": [
        "\n",
        "Now we've got a well-performing trained model, let's save it to file so we can import and use it later.\n",
        "\n",
        "To save our model we can use the [`utils.save_model()`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/utils.py) function we created in [05. PyTorch Going Modular section 5](https://www.learnpytorch.io/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy).\n",
        "\n",
        "We'll set the `target_dir` to `\"models\"` and the `model_name` to `\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"` (a little comprehensive but at least we know what's going on)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3x4RkZOaOJd",
        "outputId": "32b68610-b3de-4657-89b9-5d0bfaa15243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saving model to: models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n"
          ]
        }
      ],
      "source": [
        "from going_modular.going_modular import utils\n",
        "\n",
        "# Save the model\n",
        "utils.save_model(model=effnetb2,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCOz-uTLaOJd"
      },
      "source": [
        "### 3.6 Checking the size of EffNetB2 feature extractor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8--D7QwaOJd"
      },
      "source": [
        "\n",
        "Since one of our criteria for deploying a model to power FoodVision Mini is **speed** (~30FPS or better), let's check the size of our model.\n",
        "\n",
        "Why check the size?\n",
        "\n",
        "Well, while not always the case, the size of a model can influence its inference speed.\n",
        "\n",
        "As in, if a model has more parameters, it generally performs more operations and each one of these operations requires some computing power.\n",
        "\n",
        "And because we'd like our model to work on devices with limited computing power (e.g. on a mobile device or in a web browser), generally, the smaller the size the better (as long as it still performs well in terms of accuracy).\n",
        "\n",
        "To check our model's size in bytes, we can use Python's [`pathlib.Path.stat(\"path_to_model\").st_size`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.stat) and then we can convert it (roughly) to megabytes by dividing it by `(1024*1024)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqg-TEBQaOJd",
        "outputId": "5453a86a-f21e-4843-e164-0659e404b4dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained EffNetB2 feature extractor model size: 29 MB\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n",
        "print(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_ugLqT0aOJd"
      },
      "source": [
        "### 3.7 Collecting EffNetB2 feature extractor stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF-LOkyaaOJd"
      },
      "source": [
        "\n",
        "We've got a few statistics about our EffNetB2 feature extractor model such as test loss, test accuracy and model size, how about we collect them all in a dictionary so we can compare them to the upcoming ViT feature extractor.\n",
        "\n",
        "And we'll calculate an extra one for fun, total number of parameters.\n",
        "\n",
        "We can do so by counting the number of elements (or patterns/weights) in `effnetb2.parameters()`. We'll access the number of elements in each parameter using the [`torch.numel()`](https://pytorch.org/docs/stable/generated/torch.numel.html) (short for \"number of elements\") method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b6SUaoyaOJd",
        "outputId": "6bc1dc1e-6554-46ef-ee32-0803fceff84c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7705221"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Count number of parameters in EffNetB2\n",
        "effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\n",
        "effnetb2_total_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtaLobkZaOJd"
      },
      "source": [
        "Excellent!\n",
        "\n",
        "Now let's put everything in a dictionary so we can make comparisons later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nzgtQmTaOJd",
        "outputId": "0727c3ee-f853-45e9-ad93-9e36692e1b17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.2810868382453918,\n",
              " 'test_acc': 0.9625,\n",
              " 'number_of_parameters': 7705221,\n",
              " 'model_size (MB)': 29}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Create a dictionary with EffNetB2 statistics\n",
        "effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],   # -1 to get the last value\n",
        "                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n",
        "                  \"number_of_parameters\": effnetb2_total_params,\n",
        "                  \"model_size (MB)\": pretrained_effnetb2_model_size}\n",
        "effnetb2_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT6D7RncaOJe"
      },
      "source": [
        "Epic!\n",
        "\n",
        "Looks like our EffNetB2 model is performing at over 95% accuracy!\n",
        "\n",
        "Criteria number 1: perform at 95%+ accuracy, tick!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_jLLbpmaOJe"
      },
      "source": [
        "## 4. Creating a ViT feature extractor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKfXcZwGaOJe"
      },
      "source": [
        "\n",
        "Time to continue with our FoodVision Mini modelling experiments.\n",
        "\n",
        "This time we're going to create a ViT feature extractor.\n",
        "\n",
        "And we'll do it in much the same way as the EffNetB2 feature extractor except this time with [`torchvision.models.vit_b_16()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16) instead of `torchvision.models.efficientnet_b2()`.\n",
        "\n",
        "We'll start by creating a function called `create_vit_model()` which will be very similar to `create_effnetb2_model()` except of course returning a ViT feature extractor model and transforms rather than EffNetB2.\n",
        "\n",
        "Another slight difference is that `torchvision.models.vit_b_16()`'s output layer is called `heads` rather than `classifier`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRjzt4m4aOJe",
        "outputId": "88492324-7d04-466f-b8f7-2c46bf161cae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Check out ViT heads layer\n",
        "vit = torchvision.models.vit_b_16()\n",
        "vit.heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zCfYQWtOaOJe"
      },
      "outputs": [],
      "source": [
        "def create_vit_model(num_classes:int=3,\n",
        "                     seed:int=42):\n",
        "    \"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int, optional): number of target classes. Defaults to 3.\n",
        "        seed (int, optional): random seed value for output layer. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): ViT-B/16 feature extractor model.\n",
        "        transforms (torchvision.transforms): ViT-B/16 image transforms.\n",
        "    \"\"\"\n",
        "    # Create ViT_B_16 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.ViT_B_16_Weights.DEFAULT # weights = Vit_B_16_Weights.IMAGE_NET_1K_V1)\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.vit_b_16(weights=weights)\n",
        "\n",
        "    # Freeze all layers in model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Change classifier head to suit our needs (this will be trainable)\n",
        "    torch.manual_seed(seed)\n",
        "    model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n",
        "                                          out_features=num_classes)) # update to reflect target number of classes\n",
        "\n",
        "    return model, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4vlKxl0aOJe"
      },
      "source": [
        "ViT feature extraction model creation function ready!\n",
        "\n",
        "Let's test it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-EJLVjHaOJe",
        "outputId": "980d1dd6-fdeb-426d-f5d8-f4729a5e5fca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 330M/330M [00:02<00:00, 171MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Create ViT model and transforms\n",
        "vit, vit_transforms = create_vit_model(num_classes=3,\n",
        "                                       seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU_T37ZiaOJe"
      },
      "source": [
        "No errors, lovely to see!\n",
        "\n",
        "Now let's get a nice-looking summary of our ViT model using `torchinfo.summary()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6daDKrXaOJe",
        "outputId": "1dafa868-0343-4864-b1de-2ae3457f5c75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]     [1, 3]               768                  Partial\n",
              "‚îú‚îÄConv2d (conv_proj)                                         [1, 3, 224, 224]     [1, 768, 14, 14]     (590,592)            False\n",
              "‚îú‚îÄEncoder (encoder)                                          [1, 197, 768]        [1, 197, 768]        151,296              False\n",
              "‚îÇ    ‚îî‚îÄDropout (dropout)                                     [1, 197, 768]        [1, 197, 768]        --                   --\n",
              "‚îÇ    ‚îî‚îÄSequential (layers)                                   [1, 197, 768]        [1, 197, 768]        --                   False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_0)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_1)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_2)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_3)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_4)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_5)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_6)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_7)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_8)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_9)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_10)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_11)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
              "‚îÇ    ‚îî‚îÄLayerNorm (ln)                                        [1, 197, 768]        [1, 197, 768]        (1,536)              False\n",
              "‚îú‚îÄSequential (heads)                                         [1, 768]             [1, 3]               --                   True\n",
              "‚îÇ    ‚îî‚îÄLinear (0)                                            [1, 768]             [1, 3]               2,307                True\n",
              "============================================================================================================================================\n",
              "Total params: 85,800,963\n",
              "Trainable params: 2,307\n",
              "Non-trainable params: 85,798,656\n",
              "Total mult-adds (Units.MEGABYTES): 172.47\n",
              "============================================================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 104.09\n",
              "Params size (MB): 229.20\n",
              "Estimated Total Size (MB): 333.89\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Print ViT feature extractor model summary (uncomment for full output)\n",
        "summary(vit,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRYMsEr4aOJe"
      },
      "source": [
        "Just like our EffNetB2 feature extractor model, our ViT model's base layers are frozen and the output layer is customized to our needs!\n",
        "\n",
        "Do you notice the big difference though?\n",
        "\n",
        "Our ViT model has *far* more parameters than our EffNetB2 model. Perhaps this will come into play when we compare our models across speed and performance later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnkdplSmaOJe"
      },
      "source": [
        "### 4.1 Create DataLoaders for ViT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p47OdavaOJe"
      },
      "source": [
        "\n",
        "We've got our ViT model ready, now let's create some `DataLoader`s for it.\n",
        "\n",
        "We'll do this in the same way we did for EffNetB2 except we'll use `vit_transforms` to transform our images into the same format the ViT model was trained on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yIrAJIb8aOJe"
      },
      "outputs": [],
      "source": [
        "# Setup ViT DataLoaders\n",
        "from going_modular.going_modular import data_setup\n",
        "train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                       test_dir=test_dir,\n",
        "                                                                                       transform=vit_transforms,\n",
        "                                                                                       batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63HaTpI2aOJe"
      },
      "source": [
        "### 4.2 Training ViT feature extractor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5Y2XKmMaOJe"
      },
      "source": [
        "\n",
        "You know what time it is...\n",
        "\n",
        "...it's traininggggggg time (sung in the same tune as the song [Closing Time](https://youtu.be/xGytDsqkQY8)).\n",
        "\n",
        "Let's train our ViT feature extractor model for 10 epochs using our `engine.train()` function with `torch.optim.Adam()` and a learning rate of `1e-3` as our optimizer and `torch.nn.CrossEntropyLoss()` as our loss function.\n",
        "\n",
        "We'll use our `set_seeds()` function before training to try and make our results as reproducible as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228,
          "referenced_widgets": [
            "179c5bf44f0047daa241675ccc4af03b",
            "6b43c0866d24449eb345d3e92b3eb954",
            "bb3cf6ebc2b84eba95080d7b6efed8ec",
            "bc0c907b890345fb8e25c30e91adcfdf",
            "93ad48eb2d714e9790500cd7cd749fad",
            "2e21fe6af4474428a91637bcfc258312",
            "ca07bb3ee20a4afeb554b5bcdbfc1b6b",
            "bfea2a09502a4db3acda9c19360ab8ad",
            "392ad330efb14c419a641c1bc8f43cc1",
            "53af87cf0e0e4af5950c68874cdef435",
            "3fe3629c306e4d1b8bceb9f979ddc3d5"
          ]
        },
        "id": "FVlsUt8raOJe",
        "outputId": "65aba3b6-d4b2-4601-a525-30ae23eeada1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "179c5bf44f0047daa241675ccc4af03b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.7020 | train_acc: 0.7521 | test_loss: 0.2714 | test_acc: 0.9381\n",
            "Epoch: 2 | train_loss: 0.2532 | train_acc: 0.9062 | test_loss: 0.1672 | test_acc: 0.9602\n",
            "Epoch: 3 | train_loss: 0.1764 | train_acc: 0.9542 | test_loss: 0.1273 | test_acc: 0.9693\n",
            "Epoch: 4 | train_loss: 0.1276 | train_acc: 0.9625 | test_loss: 0.1074 | test_acc: 0.9722\n",
            "Epoch: 5 | train_loss: 0.1159 | train_acc: 0.9646 | test_loss: 0.0953 | test_acc: 0.9784\n",
            "Epoch: 6 | train_loss: 0.1274 | train_acc: 0.9375 | test_loss: 0.0832 | test_acc: 0.9722\n",
            "Epoch: 7 | train_loss: 0.0897 | train_acc: 0.9771 | test_loss: 0.0845 | test_acc: 0.9784\n",
            "Epoch: 8 | train_loss: 0.0919 | train_acc: 0.9812 | test_loss: 0.0764 | test_acc: 0.9722\n",
            "Epoch: 9 | train_loss: 0.0922 | train_acc: 0.9792 | test_loss: 0.0734 | test_acc: 0.9784\n",
            "Epoch: 10 | train_loss: 0.0658 | train_acc: 0.9833 | test_loss: 0.0644 | test_acc: 0.9847\n"
          ]
        }
      ],
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
        "                             lr=1e-3)\n",
        "# Setup loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train ViT model with seeds set for reproducibility\n",
        "set_seeds()\n",
        "vit_results = engine.train(model=vit,\n",
        "                           train_dataloader=train_dataloader_vit,\n",
        "                           test_dataloader=test_dataloader_vit,\n",
        "                           epochs=10,\n",
        "                           optimizer=optimizer,\n",
        "                           loss_fn=loss_fn,\n",
        "                           device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkXWrybtaOJf"
      },
      "source": [
        "### 4.3 Inspecting ViT loss curves\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfMVWdV7aOJf"
      },
      "source": [
        "\n",
        "Alright, alright, alright, ViT model trained, let's get visual and see some loss curves.\n",
        "\n",
        "> **Note:** Don't forget you can see what an ideal set of loss curves should look like in [04. PyTorch Custom Datasets section 8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "T-CRxTNEaOJf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "9384443e-409c-401f-8a47-cd4cec52ad13"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(vit_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gPS_4VDaOJf"
      },
      "source": [
        "Ohh yeah!\n",
        "\n",
        "Those are some nice looking loss curves. Just like our EffNetB2 feature extractor model, it looks our ViT model might benefit from a little longer training time and perhaps some [data augmentation](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation) (to help prevent overfitting)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYkWIdsVaOJf"
      },
      "source": [
        "### 4.4 Saving ViT feature extractor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTVCMeN-aOJf"
      },
      "source": [
        "\n",
        "Our ViT model is performing outstanding!\n",
        "\n",
        "So let's save it to file so we can import it and use it later if we wish.\n",
        "\n",
        "We can do so using the `utils.save_model()` function we created in [05. PyTorch Going Modular section 5](https://www.learnpytorch.io/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7MVBuuGSaOJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf2667c1-b42f-4060-84f9-c57700b5c22f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saving model to: models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "from going_modular.going_modular import utils\n",
        "\n",
        "utils.save_model(model=vit,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5G45YkKaOJf"
      },
      "source": [
        "### 4.5 Checking the size of ViT feature extractor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceC5HTC1aOJf"
      },
      "source": [
        "\n",
        "And since we want to compare our EffNetB2 model to our ViT model across a number of characteristics, let's find out its size.\n",
        "\n",
        "To check our model's size in bytes, we can use Python's `pathlib.Path.stat(\"path_to_model\").st_size` and then we can convert it (roughly) to megabytes by dividing it by `(1024*1024)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8WaN9RZ8aOJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e110694-ccb6-4371-b1d7-8e705b550ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained ViT feature extractor model size: 327 MB\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "# st_size() returns the size of the file in bytes\n",
        "pretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n",
        "print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmF5PNZdaOJf"
      },
      "source": [
        "Hmm, how does the ViT feature extractor model size compare to our EffNetB2 model size?\n",
        "\n",
        "We'll find this out shortly when we compare all of our model's characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaHTJkoQaOJf"
      },
      "source": [
        "### 4.6 Collecting ViT feature extractor stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD8YmP58aOJf"
      },
      "source": [
        "\n",
        "Let's put together all of our ViT feature extractor model statistics.\n",
        "\n",
        "We saw it in the summary output above but we'll calculate its total number of parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "jQ9tLwMgaOJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28eb956-d6e9-4d46-961e-ad9e51254ae1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85800963"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Count number of parameters in ViT\n",
        "vit_total_params = sum(torch.numel(param) for param in vit.parameters())\n",
        "vit_total_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYgWOFC-aOJf"
      },
      "source": [
        "Woah, that looks like a fair bit more than our EffNetB2!\n",
        "\n",
        "> **Note:** A larger number of parameters (or weights/patterns) generally means a model has a higher *capacity* to learn, whether it actually uses this extra capacity is another story. In light of this, our EffNetB2 model has 7,705,221 parameters where as our ViT model has 85,800,963 (11.1x more) so we could assume that our ViT model has more of a capacity to learn, if given more data (more opportunities to learn). However, this larger capacity to learn ofen comes with an  increased model filesize and a longer time to perform inference.\n",
        "\n",
        "Now let's create a dictionary with some important characteristics of our ViT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nTOEx5qkaOJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "084d1442-0667-4867-96d9-33845e464e79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.06443451717495918,\n",
              " 'test_acc': 0.984659090909091,\n",
              " 'number_of_parameters': 85800963,\n",
              " 'model_size (MB)': 327}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Create ViT statistics dictionary\n",
        "vit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n",
        "             \"test_acc\": vit_results[\"test_acc\"][-1],\n",
        "             \"number_of_parameters\": vit_total_params,\n",
        "             \"model_size (MB)\": pretrained_vit_model_size}\n",
        "\n",
        "vit_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SN_xgAOaOJg"
      },
      "source": [
        "Nice! Looks like our ViT model achieves over 95% accuracy too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_-dOTjCaOJg"
      },
      "source": [
        "## 5. Making predictions with our trained models and timing them\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMcI4sJdaOJg"
      },
      "source": [
        "\n",
        "We've got a couple of trained models, both performing pretty well.\n",
        "\n",
        "Now how about we test them out doing what we'd like them to do?\n",
        "\n",
        "As in, let's see how they go making predictions (performing inference).\n",
        "\n",
        "We know both of our models are performing at over 95% accuracy on the test dataset, but how fast are they?\n",
        "\n",
        "Ideally, if we're deploying our FoodVision Mini model to a mobile device so people can take photos of their food and identify it, we'd like the predictions to happen at real-time (~30 frames per second).\n",
        "\n",
        "That's why our second criteria is: a fast model.\n",
        "\n",
        "To find out how long each of our models take to performance inference, let's create a function called `pred_and_store()` to iterate over each of the test dataset images one by one and perform a prediction.\n",
        "\n",
        "We'll time each of the predictions as well as store the results in a common prediction format: a list of dictionaries (where each element in the list is a single prediction and each sinlge prediction is a dictionary).\n",
        "\n",
        "> **Note:** We time the predictions one by one rather than by batch because when our model is deployed, it will likely only be making a prediction on one image at a time. As in, someone takes a photo and our model predicts on that single image.\n",
        "\n",
        "Since we'd like to make predictions across all the images in the test set, let's first get a list of all of the test image paths so we can iterate over them.\n",
        "\n",
        "To do so, we'll use Python's [`pathlib.Path(\"target_dir\").glob(\"*/*.jpg\"))`](https://docs.python.org/3/library/pathlib.html#basic-use) to find all of the filepaths in a target directory with the extension `.jpg` (all of our test images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "uemrSgzfaOJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "192dd0c0-8675-4cfa-cb0f-f536499f0f8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Finding all filepaths ending with '.jpg' in directory: data/pizza_steak_sushi_20_percent/test\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('data/pizza_steak_sushi_20_percent/test/pizza/61656.jpg'),\n",
              " PosixPath('data/pizza_steak_sushi_20_percent/test/pizza/648055.jpg'),\n",
              " PosixPath('data/pizza_steak_sushi_20_percent/test/pizza/1067986.jpg'),\n",
              " PosixPath('data/pizza_steak_sushi_20_percent/test/pizza/3376617.jpg'),\n",
              " PosixPath('data/pizza_steak_sushi_20_percent/test/pizza/1032754.jpg')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get all test data paths\n",
        "print(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\")\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "test_data_paths[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn7dJEhlaOJg"
      },
      "source": [
        "### 5.1 Creating a function to make predictions across the test dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmlGW6HZaOJg"
      },
      "source": [
        "\n",
        "Now we've got a list of our test image paths, let's get to work on our `pred_and_store()` function:\n",
        "\n",
        "1. Create a function that takes a list of paths, a trained PyTorch model, a series of transforms (to prepare images), a list of target class names and a target device.\n",
        "2. Create an empty list to store prediction dictionaries (we want the function to return a list of dictionaries, one for each prediction).\n",
        "3. Loop through the target input paths (steps 4-14 will happen inside the loop).\n",
        "4. Create an empty dictionary for each iteration in the loop to store prediction values per sample.\n",
        "5. Get the sample path and ground truth class name (we can do this by inferring the class from the path).\n",
        "6. Start the prediction timer using Python's [`timeit.default_timer()`](https://docs.python.org/3/library/timeit.html#timeit.default_timer).\n",
        "7. Open the image using [`PIL.Image.open(path)`](https://pillow.readthedocs.io/en/stable/reference/Image.html#functions).\n",
        "8. Transform the image so it's capable of being used with the target model as well as add a batch dimension and send the image to the target device.\n",
        "9. Prepare the model for inference by sending it to the target device and turning on `eval()` mode.\n",
        "10. Turn on [`torch.inference_mode()`](https://pytorch.org/docs/stable/generated/torch.inference_mode.html) and pass the target transformed image to the model and calculate the prediction probability using `torch.softmax()` and the target label using `torch.argmax()`.\n",
        "11. Add the prediction probability and prediction class to the prediction dictionary created in step 4. Also make sure the prediction probability is on the CPU so it can be used with non-GPU libraries such as NumPy and pandas for later inspection.\n",
        "12. End the prediction timer started in step 6 and add the time to the prediction dictionary created in step 4.\n",
        "13. See if the predicted class matches the ground truth class from step 5 and add the result to the prediction dictionary created in step 4.\n",
        "14. Append the updated prediction dictionary to the empty list of predictions created in step 2.\n",
        "15. Return the list of prediction dictionaries.\n",
        "\n",
        "A bunch of steps, but nothing we can't handle!\n",
        "\n",
        "Let's do it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "76jemr7OaOJg"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List, Dict\n",
        "\n",
        "# 1. Create a function to return a list of dictionaries with sample, truth label, prediction, prediction probability and prediction time\n",
        "def pred_and_store(paths: List[pathlib.Path],\n",
        "                   model: torch.nn.Module,\n",
        "                   transform: torchvision.transforms,\n",
        "                   class_names: List[str],\n",
        "                   device: str = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\") -> List[Dict]:\n",
        "\n",
        "    # 2. Create an empty list to store prediction dictionaries\n",
        "    pred_list = []\n",
        "\n",
        "    # 3. Loop through target paths\n",
        "    for path in tqdm(paths):\n",
        "\n",
        "        # 4. Create empty dictionary to store prediction information for each sample\n",
        "        pred_dict = {}\n",
        "\n",
        "        # 5. Get the sample path and ground truth class name\n",
        "        pred_dict[\"image_path\"] = path\n",
        "        class_name = path.parent.stem\n",
        "        pred_dict[\"class_name\"] = class_name\n",
        "\n",
        "        # 6. Start the prediction timer\n",
        "        start_time = timer()\n",
        "\n",
        "        # 7. Open image path\n",
        "        img = Image.open(path)\n",
        "\n",
        "        # 8. Transform the image, add batch dimension and put image on target device\n",
        "        transformed_image = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "        # 9. Prepare model for inference by sending it to target device and turning on eval() mode\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        # 10. Get prediction probability, predicition label and prediction class\n",
        "        with torch.inference_mode():\n",
        "            pred_logit = model(transformed_image) # perform inference on target sample\n",
        "            pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities\n",
        "            pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label\n",
        "            pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU\n",
        "\n",
        "            # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on)\n",
        "            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
        "            pred_dict[\"pred_class\"] = pred_class\n",
        "\n",
        "            # 12. End the timer and calculate time per pred\n",
        "            end_time = timer()\n",
        "            pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n",
        "\n",
        "        # 13. Does the pred match the true label?\n",
        "        pred_dict[\"correct\"] = class_name == pred_class\n",
        "\n",
        "        # 14. Add the dictionary to the list of preds\n",
        "        pred_list.append(pred_dict)\n",
        "\n",
        "    # 15. Return list of prediction dictionaries\n",
        "    return pred_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_K4tQbfaOJg"
      },
      "source": [
        "**Function Overview: `pred_and_store`**  \n",
        "\n",
        "- This function takes a list of image paths, a PyTorch model, a transform, class names, and a target device, then performs predictions on each image, storing results in a structured format. Below is a detailed breakdown:\n",
        "\n",
        "---\n",
        "\n",
        "**1. Function Signature**  \n",
        "```python\n",
        "def pred_and_store(paths: List[pathlib.Path], model: torch.nn.Module, transform: torchvision.transforms, class_names: List[str], device: str = ...) -> List[Dict]:\n",
        "```\n",
        "- **`List[pathlib.Path]`**:  \n",
        "  - A type hint indicating `paths` is a list of `pathlib.Path` objects (file paths).  \n",
        "  - `pathlib.Path` is Python's object-oriented way to handle filesystem paths (e.g., `/data/test/dog.jpg`).  \n",
        "- **Return Type**: A list of dictionaries (`List[Dict]`), where each dictionary holds prediction details for one image.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Initialize `pred_list`**  \n",
        "```python\n",
        "pred_list = []\n",
        "```\n",
        "- **Purpose**: Stores prediction results for all images.  \n",
        "- **How it grows**: Each iteration appends a `pred_dict` (one per image) to this list.\n",
        "\n",
        "---\n",
        "\n",
        "**3. Loop Through Image Paths**  \n",
        "```python\n",
        "for path in tqdm(paths):\n",
        "```\n",
        "- **`tqdm`**: Wraps the loop to show a progress bar for long-running operations.\n",
        "\n",
        "---\n",
        "\n",
        "**4. Create `pred_dict` for Each Image**  \n",
        "```python\n",
        "pred_dict = {}\n",
        "```\n",
        "- **Purpose**: Stores metadata and results for a single image.  \n",
        "- **Fields Added**:  \n",
        "  - `image_path`: Path to the image.  \n",
        "  - `class_name`: Ground truth label (extracted from parent folder name via `path.parent.stem`).  \n",
        "\n",
        "---\n",
        "\n",
        "**5. Timer Setup**  \n",
        "```python\n",
        "start_time = timer()\n",
        "```\n",
        "- **Purpose**: Measures prediction time for benchmarking.\n",
        "\n",
        "---\n",
        "\n",
        "**6. Load and Transform Image**  \n",
        "```python\n",
        "img = Image.open(path)\n",
        "transformed_image = transform(img).unsqueeze(0).to(device)\n",
        "```\n",
        "- **`transform(img)`**: Applies preprocessing (e.g., resize, normalize) to match model training.  \n",
        "- **`unsqueeze(0)`**:  \n",
        "  - Adds a batch dimension (dim=0) to the tensor.  \n",
        "  - **Why?**: Models expect inputs in batches (e.g., shape `[1, 3, 224, 224]` for a single RGB image).  \n",
        "- **`.to(device)`**: Moves the tensor to CPU/GPU for computation.\n",
        "\n",
        "---\n",
        "\n",
        "**7. Model Inference Setup**  \n",
        "```python\n",
        "model.to(device)\n",
        "model.eval()\n",
        "```\n",
        "- **`model.to(device)`**: Ensures the model is on the same device as the input tensor.  \n",
        "- **`model.eval()`**:  \n",
        "  - Disables dropout/batch norm layers (not needed for inference).  \n",
        "  - Equivalent to `model.train(False)`.\n",
        "\n",
        "---\n",
        "\n",
        "**8. Prediction Logic**  \n",
        "```python\n",
        "with torch.inference_mode():\n",
        "    pred_logit = model(transformed_image)\n",
        "    pred_prob = torch.softmax(pred_logit, dim=1)\n",
        "    pred_label = torch.argmax(pred_prob, dim=1)\n",
        "    pred_class = class_names[pred_label.cpu()]\n",
        "```\n",
        "- **`inference_mode()`**:  \n",
        "  - Disables gradient tracking (faster inference, less memory).  \n",
        "  - Like `torch.no_grad()` but optimized further.  \n",
        "- **`softmax`**:  \n",
        "  - Converts logits (raw model outputs) to probabilities (sum to 1).  \n",
        "  - **Example**: Logits `[2.0, 1.0]` ‚Üí Probabilities `[0.73, 0.27]`.  \n",
        "- **`argmax`**:  \n",
        "  - Returns the index of the highest probability.  \n",
        "  - **Example**: Probabilities `[0.9, 0.1]` ‚Üí Label `0`.  \n",
        "- **`class_names[pred_label.cpu()]`**: Maps the label index to a class name (e.g., `0` ‚Üí \"dog\").\n",
        "\n",
        "---\n",
        "\n",
        "**9. Store Results in `pred_dict`**  \n",
        "```python\n",
        "pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
        "pred_dict[\"pred_class\"] = pred_class\n",
        "pred_dict[\"time_for_pred\"] = round(end_time - start_time, 4)\n",
        "pred_dict[\"correct\"] = class_name == pred_class\n",
        "```\n",
        "- **`pred_prob.unsqueeze(0).max().cpu().item()`**:  \n",
        "  - `unsqueeze(0)`: Ensures tensor has a batch dimension (if not already).  \n",
        "  - `max()`: Extracts the highest probability value.  \n",
        "  - `cpu().item()`: Moves to CPU and converts to a Python float.  \n",
        "  - **Why?**: Ensures compatibility for non-GPU environments and rounding.  \n",
        "- **`correct`**: Boolean flag for prediction accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "**10. Append to `pred_list`**  \n",
        "```python\n",
        "pred_list.append(pred_dict)\n",
        "```\n",
        "- **How it grows**: Each `pred_dict` is added to `pred_list`, creating a list of dictionaries for analysis.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Concepts Recap**  \n",
        "- **Batch Processing**: Models expect batched inputs; `unsqueeze(0)` simulates a batch of size 1.  \n",
        "- **Device Management**: Tensors and models must be on the same device (CPU/GPU).  \n",
        "- **Probability Conversion**: `softmax` + `argmax` turns logits ‚Üí class names.  \n",
        "- **Performance**: `inference_mode()` and `eval()` optimize prediction speed.  \n",
        "\n",
        "This function is a utility for structured prediction logging, useful for model evaluation and deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEYySGw1aOJg"
      },
      "source": [
        "Ho, ho!\n",
        "\n",
        "What a good looking function!\n",
        "\n",
        "And you know what, since our `pred_and_store()` is a pretty good utility function for making and storing predictions, it could be stored to [`going_modular.going_modular.predictions.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/predictions.py) for later use. That might be an extension you'd like to try, check out [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/) for ideas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRiM2uuTaOJg"
      },
      "source": [
        "### 5.2 Making and timing predictions with EffNetB2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmDIeBIMaOJg"
      },
      "source": [
        "\n",
        "Time to test out our `pred_and_store()` function!\n",
        "\n",
        "Let's start by using it to make predictions across the test dataset with our EffNetB2 model, paying attention to two details:\n",
        "\n",
        "1. **Device** - We'll hard code the `device` parameter to use `\"cpu\"` because when we deploy our model, we won't always have access to a `\"cuda\"` (GPU) device.\n",
        "    * Making the predictions on CPU will be a good indicator of speed of inference too because generally predictions on CPU devices are slower than GPU devices.\n",
        "2. **Transforms** - We'll also be sure to set the `transform` parameter to `effnetb2_transforms` to make sure the images are opened and transformed in the same way our `effnetb2` model has been trained on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "lJFxAn4GaOJg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7adc20e6d55542c483ae2da5a98c587e",
            "add56e5405114eb89fe02471c57e7899",
            "d758865bf2664c6ea31248cc843974f9",
            "3ca11549641e47aa99975574a9f285bb",
            "328a7757d034423cb8394715a90b3e4c",
            "296878d548f844ba8bae2c300354f707",
            "37c9514871e64a68a2e7e075c31804ba",
            "b3cfab2ffeee45589c8283c1b4abe12c",
            "dfaadaf997e14cf09c517d7ba8775601",
            "f1b6a2273be944399bbf661b37812fd8",
            "9a2e09eeafa3440a9b5b5f356b00f14d"
          ]
        },
        "outputId": "03cbcd69-33ee-441b-8182-09eccddcf029"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/150 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7adc20e6d55542c483ae2da5a98c587e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# Make predictions across test dataset with EffNetB2\n",
        "effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,    # list of test data paths\n",
        "                                          model=effnetb2,   # model to use for inference\n",
        "                                          transform=effnetb2_transforms,    # transforms to use for inference\n",
        "                                          class_names=class_names,  # class names for the dataset\n",
        "                                          device=\"cpu\") # make predictions on CPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjsw0ENfaOJh"
      },
      "source": [
        "Nice! Look at those predictions fly!\n",
        "\n",
        "Let's inspect the first couple and see what they look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "RK2pg0m3aOJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a8db7ca-2cb7-41f7-bbc1-6be5d3b81521"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/pizza/61656.jpg'),\n",
              "  'class_name': 'pizza',\n",
              "  'pred_prob': 0.625,\n",
              "  'pred_class': 'pizza',\n",
              "  'time_for_pred': 0.3061,\n",
              "  'correct': True},\n",
              " {'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/pizza/648055.jpg'),\n",
              "  'class_name': 'pizza',\n",
              "  'pred_prob': 0.8625,\n",
              "  'pred_class': 'pizza',\n",
              "  'time_for_pred': 0.1223,\n",
              "  'correct': True}]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# Inspect the first 2 prediction dictionaries\n",
        "effnetb2_test_pred_dicts[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CXMwO-qaOJh"
      },
      "source": [
        "Woohoo!\n",
        "\n",
        "It looks like our `pred_and_store()` function worked nicely.\n",
        "\n",
        "Thanks to our list of dictionaries data structure, we've got plenty of useful information we can further inspect.\n",
        "\n",
        "To do so, let's turn our list of dictionaries into a pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LG3tFA_OaOJh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "0e5eea27-fba7-46de-b704-910669fd28b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          image_path class_name  pred_prob  \\\n",
              "0  data/pizza_steak_sushi_20_percent/test/pizza/6...      pizza     0.6250   \n",
              "1  data/pizza_steak_sushi_20_percent/test/pizza/6...      pizza     0.8625   \n",
              "2  data/pizza_steak_sushi_20_percent/test/pizza/1...      pizza     0.9919   \n",
              "3  data/pizza_steak_sushi_20_percent/test/pizza/3...      pizza     0.5670   \n",
              "4  data/pizza_steak_sushi_20_percent/test/pizza/1...      pizza     0.4744   \n",
              "\n",
              "  pred_class  time_for_pred  correct  \n",
              "0      pizza         0.3061     True  \n",
              "1      pizza         0.1223     True  \n",
              "2      pizza         0.1081     True  \n",
              "3      pizza         0.1114     True  \n",
              "4      pizza         0.1244     True  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0b0fd364-23bb-4c47-b7ec-1e5f4f7865de\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>class_name</th>\n",
              "      <th>pred_prob</th>\n",
              "      <th>pred_class</th>\n",
              "      <th>time_for_pred</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/pizza/6...</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.6250</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.3061</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/pizza/6...</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.8625</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.1223</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/pizza/1...</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.9919</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.1081</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/pizza/3...</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.5670</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.1114</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/pizza/1...</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.4744</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.1244</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b0fd364-23bb-4c47-b7ec-1e5f4f7865de')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0b0fd364-23bb-4c47-b7ec-1e5f4f7865de button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0b0fd364-23bb-4c47-b7ec-1e5f4f7865de');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-86505297-0b9c-4af6-b942-1074a30321de\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-86505297-0b9c-4af6-b942-1074a30321de')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-86505297-0b9c-4af6-b942-1074a30321de button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "effnetb2_test_pred_df",
              "summary": "{\n  \"name\": \"effnetb2_test_pred_df\",\n  \"rows\": 150,\n  \"fields\": [\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 150,\n        \"samples\": [\n          \"data/pizza_steak_sushi_20_percent/test/steak/476333.jpg\",\n          \"data/pizza_steak_sushi_20_percent/test/pizza/3174637.jpg\",\n          \"data/pizza_steak_sushi_20_percent/test/sushi/472912.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"pizza\",\n          \"steak\",\n          \"sushi\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_prob\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15655144607590307,\n        \"min\": 0.3734,\n        \"max\": 0.9941,\n        \"num_unique_values\": 147,\n        \"samples\": [\n          0.9504,\n          0.633,\n          0.6722\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"pizza\",\n          \"sushi\",\n          \"steak\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time_for_pred\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02133723946342317,\n        \"min\": 0.0831,\n        \"max\": 0.3061,\n        \"num_unique_values\": 121,\n        \"samples\": [\n          0.1012,\n          0.1394,\n          0.1244\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# Turn the test_pred_dicts into a DataFrame\n",
        "import pandas as pd\n",
        "effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)  # turns the list of dictionaries into a DataFrame\n",
        "effnetb2_test_pred_df.head()    # inspect the first 5 rows of the DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B20xXYRDaOJh"
      },
      "source": [
        "Beautiful!\n",
        "\n",
        "Look how easily those prediction dictionaries turn into a structured format we can perform analysis on.\n",
        "\n",
        "Such as finding how many predictions our EffNetB2 model got wrong..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "CyQztlWoaOJh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "a6280351-7e74-4a8c-c5b2-ca72fa29b965"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "correct\n",
              "True     144\n",
              "False      6\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>correct</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>True</th>\n",
              "      <td>144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>False</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# Check number of correct predictions\n",
        "effnetb2_test_pred_df.correct.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6xyVQHUaOJh"
      },
      "source": [
        "\n",
        "**1. `.correct`**  \n",
        "- **Purpose**: Accesses the `correct` column in the DataFrame `effnetb2_test_pred_df`.  \n",
        "- **Context**:  \n",
        "  - The DataFrame likely contains prediction results from a PyTorch model (e.g., generated by a function like `pred_and_store`).  \n",
        "  - The `correct` column is a **Boolean field** (`True`/`False`) indicating whether the model's prediction matched the ground truth label for each image.  \n",
        "  - Example row:  \n",
        "    ```python\n",
        "    {\"image_path\": \"test/dog.jpg\", \"pred_class\": \"dog\", \"correct\": True, ...}\n",
        "    ```\n",
        "\n",
        "**2. `.value_counts()`**  \n",
        "- **Purpose**: Counts the occurrences of unique values in the `correct` column.  \n",
        "- **Output**:  \n",
        "  - Returns a Pandas `Series` showing how many `True` (correct predictions) and `False` (incorrect predictions) exist.  \n",
        "  - Example output:  \n",
        "    ```python\n",
        "    True     85\n",
        "    False    15\n",
        "    Name: correct, dtype: int64\n",
        "    ```\n",
        "- **Use Case**:  \n",
        "  - Quickly evaluate model accuracy by comparing the ratio of `True` to `False`.  \n",
        "  - For example, `85 correct / (85 + 15) = 85% accuracy`.\n",
        "\n",
        "**3. `.correct.value_counts()`**  \n",
        "The line:  \n",
        "```python\n",
        "effnetb2_test_pred_df.correct.value_counts()\n",
        "```  \n",
        "1. Selects the `correct` column from the DataFrame.  \n",
        "2. Counts how many predictions were correct (`True`) vs. incorrect (`False`).  \n",
        "3. Returns a summary of model performance (e.g., \"85 correct, 15 wrong\").  \n",
        "\n",
        "This is a common way to **benchmark classification models** in machine learning workflows.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12oaFwGuaOJh"
      },
      "source": [
        "Five wrong predictions out of 150 total, not bad!\n",
        "\n",
        "And how about the average prediction time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "SFlTNlT0aOJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ceeb06-41dc-4922-fe9c-a23741d3c55b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EffNetB2 average time per prediction: 0.104 seconds\n"
          ]
        }
      ],
      "source": [
        "# Find the average time per prediction\n",
        "effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\n",
        "print(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut8fDaH7aOJh"
      },
      "source": [
        "Hmm, how does that average prediction time live up to our criteria of our model performing at real-time (~30FPS or 0.03 seconds per prediction)?\n",
        "\n",
        "> **Note:** Prediction times will be different across different hardware types (e.g. a local Intel i9 vs Google Colab CPU). The better and faster the hardware, generally, the faster the prediction. For example, on my local deep learning PC with an Intel i9 chip, my average prediction time with EffNetB2 is around 0.031 seconds (just under real-time). However, on Google Colab (I'm not sure what CPU hardware Colab uses but it looks like it might be an [Intel(R) Xeon(R)](https://stackoverflow.com/questions/47805170/whats-the-hardware-spec-for-google-colaboratory)), my average prediction time with EffNetB2 is about 0.1396 seconds (3-4x slower).\n",
        "\n",
        "Let's add our EffNetB2 average time per prediction to our `effnetb2_stats` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qp1qDE1kaOJh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8db9f16-8239-4adf-fba3-bcbb57ee8fce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.2810868382453918,\n",
              " 'test_acc': 0.9625,\n",
              " 'number_of_parameters': 7705221,\n",
              " 'model_size (MB)': 29,\n",
              " 'time_per_pred_cpu': np.float64(0.104)}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# Add EffNetB2 average prediction time to stats dictionary\n",
        "effnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\n",
        "effnetb2_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCRTEdjwaOJh"
      },
      "source": [
        "### 5.3 Making and timing predictions with ViT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UyUJMV-aOJh"
      },
      "source": [
        "\n",
        "We've made predictions with our EffNetB2 model, now let's do the same for our ViT model.\n",
        "\n",
        "To do so, we can use the `pred_and_store()` function we created above except this time we'll pass in our `vit` model as well as the `vit_transforms`.\n",
        "\n",
        "And we'll keep the predictions on the CPU via `device=\"cpu\"` (a natural extension here would be to test the prediction times on CPU and on GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "QNqIBw9waOJh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6bce3cafd24147b19f5f0dce1642ae44",
            "15fb718b8ed14bb183f26cf5f08d4fc8",
            "45f7ef58c26b4ddc856e93a4d4367cec",
            "4508699204cc434286c1fa917e8090dc",
            "d58d29251ebb4e93b4d659e94150e487",
            "c29a2066a97e45a783d064608a5dc7e5",
            "ad70dfc51554473f8053c093c64aefde",
            "0daddfb3531344e78dacd724aa2a225a",
            "329a75043fd44d078c4c4e67509c2c82",
            "e1f26247a8d64a8e9279c6cfcbed9402",
            "bf18eb40084743d68eb4383f2937bd03"
          ]
        },
        "outputId": "e81d8ab3-d6cb-4abd-9a44-bf1bb61acc05"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/150 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bce3cafd24147b19f5f0dce1642ae44"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Make list of prediction dictionaries with ViT feature extractor model on test images\n",
        "vit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
        "                                     model=vit,\n",
        "                                     transform=vit_transforms,\n",
        "                                     class_names=class_names,\n",
        "                                     device=\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLH4cqxMaOJi"
      },
      "source": [
        "Predictions made!\n",
        "\n",
        "Now let's check out the first couple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "6MD4qeFPaOJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f273bb8d-3e5b-4f83-c0e1-61ff4a112e4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/pizza/61656.jpg'),\n",
              "  'class_name': 'pizza',\n",
              "  'pred_prob': 0.9855,\n",
              "  'pred_class': 'pizza',\n",
              "  'time_for_pred': 0.5378,\n",
              "  'correct': True},\n",
              " {'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/pizza/648055.jpg'),\n",
              "  'class_name': 'pizza',\n",
              "  'pred_prob': 0.9969,\n",
              "  'pred_class': 'pizza',\n",
              "  'time_for_pred': 0.3556,\n",
              "  'correct': True}]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# Check the first couple of ViT predictions on the test dataset\n",
        "vit_test_pred_dicts[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYrKqVa9aOJi"
      },
      "source": [
        "Wonderful!\n",
        "\n",
        "And just like before, since our ViT model's predictions are in the form of a list of dictionaries, we can easily turn them into a pandas DataFrame for further inspection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "9Ij6mlEfaOJi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "595dcd3c-bfb6-45ec-9c8a-59893f1d0cd1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          image_path class_name  pred_prob  \\\n",
              "0  data/pizza_steak_sushi_20_percent/test/pizza/6...      pizza     0.9855   \n",
              "1  data/pizza_steak_sushi_20_percent/test/pizza/6...      pizza     0.9969   \n",
              "2  data/pizza_steak_sushi_20_percent/test/pizza/1...      pizza     0.9987   \n",
              "3  data/pizza_steak_sushi_20_percent/test/pizza/3...      pizza     0.9968   \n",
              "4  data/pizza_steak_sushi_20_percent/test/pizza/1...      pizza     0.9957   \n",
              "\n",
              "  pred_class  time_for_pred  correct  \n",
              "0      pizza         0.5378     True  \n",
              "1      pizza         0.3556     True  \n",
              "2      pizza         0.3547     True  \n",
              "3      pizza         0.3643     True  \n",
              "4      pizza         0.3997     True  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7f5a4813-41c0-445a-b99d-d764ab11b048\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>class_name</th>\n",
              "      <th>pred_prob</th>\n",
              "      <th>pred_class</th>\n",
              "      <th>time_for_pred</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/pizza/6...</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.9855</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.5378</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/pizza/6...</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.9969</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.3556</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/pizza/1...</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.9987</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.3547</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/pizza/3...</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.9968</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.3643</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/pizza/1...</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.9957</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.3997</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f5a4813-41c0-445a-b99d-d764ab11b048')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7f5a4813-41c0-445a-b99d-d764ab11b048 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7f5a4813-41c0-445a-b99d-d764ab11b048');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7979b4fc-b5c2-46e7-b860-04014c475ff3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7979b4fc-b5c2-46e7-b860-04014c475ff3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7979b4fc-b5c2-46e7-b860-04014c475ff3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "vit_test_pred_df",
              "summary": "{\n  \"name\": \"vit_test_pred_df\",\n  \"rows\": 150,\n  \"fields\": [\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 150,\n        \"samples\": [\n          \"data/pizza_steak_sushi_20_percent/test/steak/476333.jpg\",\n          \"data/pizza_steak_sushi_20_percent/test/pizza/3174637.jpg\",\n          \"data/pizza_steak_sushi_20_percent/test/sushi/472912.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"pizza\",\n          \"steak\",\n          \"sushi\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_prob\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1073135066879905,\n        \"min\": 0.4739,\n        \"max\": 0.999,\n        \"num_unique_values\": 116,\n        \"samples\": [\n          0.878,\n          0.9957,\n          0.9194\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"pizza\",\n          \"steak\",\n          \"sushi\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time_for_pred\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05896362901237275,\n        \"min\": 0.3336,\n        \"max\": 0.5419,\n        \"num_unique_values\": 136,\n        \"samples\": [\n          0.3602,\n          0.3607,\n          0.3573\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# Turn vit_test_pred_dicts into a DataFrame\n",
        "import pandas as pd\n",
        "vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\n",
        "vit_test_pred_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqB2ppMiaOJi"
      },
      "source": [
        "How many predictions did our ViT model get correct?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "0-gA3DNIaOJi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "dc62b882-6fc3-4207-fff9-258c3b5c1cd7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "correct\n",
              "True     148\n",
              "False      2\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>correct</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>True</th>\n",
              "      <td>148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>False</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "# Count the number of correct predictions\n",
        "vit_test_pred_df.correct.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssqi6HL3aOJi"
      },
      "source": [
        "Woah!\n",
        "\n",
        "Our ViT model did a little better than our EffNetB2 model in terms of correct predictions, only two samples wrong across the whole test dataset.\n",
        "\n",
        "As an extension you might want to visualize the ViT model's wrong predictions and see if there's any reason why it might've got them wrong.\n",
        "\n",
        "How about we calculate how long the ViT model took per prediction?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "c8egKf6qaOJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d9a627-2048-495e-c368-94bd1166be3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ViT average time per prediction: 0.383 seconds\n"
          ]
        }
      ],
      "source": [
        "# Calculate average time per prediction for ViT model\n",
        "vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\n",
        "print(f\"ViT average time per prediction: {vit_average_time_per_pred} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUL-c-dnaOJi"
      },
      "source": [
        "Well, that looks a little slower than our EffNetB2 model's average time per prediction but how does it look in terms of our second criteria: speed?\n",
        "\n",
        "For now, let's add the value to our `vit_stats` dictionary so we can compare it to our EffNetB2 model's stats.\n",
        "\n",
        "> **Note:** The average time per prediction values will be highly dependent on the hardware you make them on. For example, for the ViT model, my average time per prediction (on the CPU) was 0.0693-0.0777 seconds on my local deep learning PC with an Intel i9 CPU. Where as on Google Colab, my average time per prediction with the ViT model was 0.6766-0.7113 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "557-8xsVaOJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7d03839-c8a6-4a1d-db75-c7bcf3f19c92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.06443451717495918,\n",
              " 'test_acc': 0.984659090909091,\n",
              " 'number_of_parameters': 85800963,\n",
              " 'model_size (MB)': 327,\n",
              " 'time_per_pred_cpu': np.float64(0.383)}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# Add average prediction time for ViT model on CPU\n",
        "vit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\n",
        "vit_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEH12QR8aOJi"
      },
      "source": [
        "## 6. Comparing model results, prediction times and size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZilAFoeLaOJi"
      },
      "source": [
        "\n",
        "Our two best model contenders have been trained and evaluated.\n",
        "\n",
        "Now let's put them head to head and compare across their different statistics.\n",
        "\n",
        "To do so, let's turn our `effnetb2_stats` and `vit_stats` dictionaries into a pandas DataFrame.\n",
        "\n",
        "We'll add a column to view the model names as well as convert the test accuracy to a whole percentage rather than decimal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "W0v-fI7oaOJi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "f2a76771-b672-4ae5-d8f4-b01d7bb35792"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   test_loss  test_acc  number_of_parameters  model_size (MB)  \\\n",
              "0   0.281087     96.25               7705221               29   \n",
              "1   0.064435     98.47              85800963              327   \n",
              "\n",
              "   time_per_pred_cpu     model  \n",
              "0              0.104  EffNetB2  \n",
              "1              0.383       ViT  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-468e4696-7122-4f54-a180-9ea067ce8a4c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>test_loss</th>\n",
              "      <th>test_acc</th>\n",
              "      <th>number_of_parameters</th>\n",
              "      <th>model_size (MB)</th>\n",
              "      <th>time_per_pred_cpu</th>\n",
              "      <th>model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.281087</td>\n",
              "      <td>96.25</td>\n",
              "      <td>7705221</td>\n",
              "      <td>29</td>\n",
              "      <td>0.104</td>\n",
              "      <td>EffNetB2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.064435</td>\n",
              "      <td>98.47</td>\n",
              "      <td>85800963</td>\n",
              "      <td>327</td>\n",
              "      <td>0.383</td>\n",
              "      <td>ViT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-468e4696-7122-4f54-a180-9ea067ce8a4c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-468e4696-7122-4f54-a180-9ea067ce8a4c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-468e4696-7122-4f54-a180-9ea067ce8a4c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3caa7f0f-eca5-420f-85cc-37923e65836b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3caa7f0f-eca5-420f-85cc-37923e65836b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3caa7f0f-eca5-420f-85cc-37923e65836b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_73d5d599-ca10-4f42-bed3-5fa9c8a5b4f6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_73d5d599-ca10-4f42-bed3-5fa9c8a5b4f6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"test_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15319632538870806,\n        \"min\": 0.06443451717495918,\n        \"max\": 0.2810868382453918,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.06443451717495918,\n          0.2810868382453918\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.5697770542341347,\n        \"min\": 96.25,\n        \"max\": 98.47,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          98.47,\n          96.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_of_parameters\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 55222028,\n        \"min\": 7705221,\n        \"max\": 85800963,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          85800963,\n          7705221\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_size (MB)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 210,\n        \"min\": 29,\n        \"max\": 327,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          327,\n          29\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time_per_pred_cpu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19728279195104678,\n        \"min\": 0.104,\n        \"max\": 0.383,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.383,\n          0.104\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"ViT\",\n          \"EffNetB2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# Turn stat dictionaries into DataFrame\n",
        "df = pd.DataFrame([effnetb2_stats, vit_stats])\n",
        "\n",
        "# Add column for model names\n",
        "df[\"model\"] = [\"EffNetB2\", \"ViT\"]\n",
        "\n",
        "# Convert accuracy to percentages\n",
        "df[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUyQ4Th4aOJi"
      },
      "source": [
        "Wonderful!\n",
        "\n",
        "It seems our models are quite close in terms of overall test accuracy but how do they look across the other fields?\n",
        "\n",
        "One way to find out would be to divide the ViT model statistics by the EffNetB2 model statistics to find out the different ratios between the models.\n",
        "\n",
        "Let's create another DataFrame to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "FbOLf3hBaOJi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "ad25b6c8-292a-429b-ffa5-08e9774bf3f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        test_loss  test_acc  number_of_parameters  \\\n",
              "ViT to EffNetB2 ratios   0.229233  1.023065             11.135432   \n",
              "\n",
              "                        model_size (MB)  time_per_pred_cpu  \n",
              "ViT to EffNetB2 ratios        11.275862           3.682692  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0fb26086-e092-4115-97d5-876036cc642d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>test_loss</th>\n",
              "      <th>test_acc</th>\n",
              "      <th>number_of_parameters</th>\n",
              "      <th>model_size (MB)</th>\n",
              "      <th>time_per_pred_cpu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ViT to EffNetB2 ratios</th>\n",
              "      <td>0.229233</td>\n",
              "      <td>1.023065</td>\n",
              "      <td>11.135432</td>\n",
              "      <td>11.275862</td>\n",
              "      <td>3.682692</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0fb26086-e092-4115-97d5-876036cc642d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0fb26086-e092-4115-97d5-876036cc642d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0fb26086-e092-4115-97d5-876036cc642d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"             columns=[\\\"ViT to EffNetB2 ratios\\\"])\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"test_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.22923349089261574,\n        \"max\": 0.22923349089261574,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.22923349089261574\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.023064935064935,\n        \"max\": 1.023064935064935,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.023064935064935\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_of_parameters\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 11.135431806563368,\n        \"max\": 11.135431806563368,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          11.135431806563368\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_size (MB)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 11.275862068965518,\n        \"max\": 11.275862068965518,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          11.275862068965518\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time_per_pred_cpu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 3.682692307692308,\n        \"max\": 3.682692307692308,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3.682692307692308\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# Compare ViT to EffNetB2 across different characteristics\n",
        "pd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]), # divide ViT statistics by EffNetB2 statistics\n",
        "             columns=[\"ViT to EffNetB2 ratios\"]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjDxs7w8aOJi"
      },
      "source": [
        "In the code above, the goal is to **compare the performance and characteristics of two PyTorch models (ViT and EffNetB2)** by calculating ratios between their metrics. Here's a breakdown of the syntax and logic:\n",
        "\n",
        "- **Step-by-Step Breakdown**:\n",
        "  1. **`df.set_index(\"model\")`**  \n",
        "    - Converts the `model` column (containing `\"EffNetB2\"` and `\"ViT\"`) into the DataFrame's index.  \n",
        "    - Example transformed `df`:\n",
        "      ```\n",
        "                  test_acc  test_loss  ...  model_size_mb  \n",
        "      model                                \n",
        "      EffNetB2     85.00      0.321  ...         29.0  \n",
        "      ViT          98.67      0.074  ...        327.0  \n",
        "      ```\n",
        "\n",
        "  2. **`.loc[\"ViT\"]` and `.loc[\"EffNetB2\"]`**  \n",
        "    - Selects the row corresponding to the **ViT model** and **EffNetB2 model** respectively.  \n",
        "    - Each returns a Pandas `Series` with model statistics (e.g., `test_acc`, `test_loss`, `model_size_mb`).\n",
        "\n",
        "  3. **Division (`ViT_Series / EffNetB2_Series`)**  \n",
        "    - Computes **element-wise division** of ViT's metrics by EffNetB2's metrics.  \n",
        "    - Example output (ratios):\n",
        "      ```\n",
        "      test_acc            1.16  # ViT is 1.16x more accurate\n",
        "      test_loss           0.23  # ViT's loss is 0.23x (lower is better)\n",
        "      model_size_mb      11.28  # ViT is 11.28x larger\n",
        "      time_per_pred_cpu   2.53  # ViT is 2.53x slower\n",
        "      ```\n",
        "\n",
        "  4. **`pd.DataFrame(..., columns=[\"ViT to EffNetB2 ratios\"])`**  \n",
        "    - Wraps the ratios in a DataFrame with a descriptive column name.\n",
        "\n",
        "  5. **`.T` (Transpose)**  \n",
        "    - Switches rows and columns for better readability. Final output:\n",
        "      ```\n",
        "                    test_acc  test_loss  model_size_mb  time_per_pred_cpu\n",
        "      ViT to EffNetB2 ratios      1.16       0.23          11.28              2.53\n",
        "      ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUbzNQ4qaOJi"
      },
      "source": [
        "It seems our ViT model outperforms the EffNetB2 model across the performance metrics (test loss, where lower is better and test accuracy, where higher is better) but at the expense of having:\n",
        "* 11x+ the number of parameters.\n",
        "* 11x+ the model size.\n",
        "* 2.5x+ the prediction time per image.\n",
        "\n",
        "Are these tradeoffs worth it?\n",
        "\n",
        "Perhaps if we had unlimited compute power but for our use case of deploying the FoodVision Mini model to a smaller device (e.g. a mobile phone), we'd likely start out with the EffNetB2 model for faster predictions at a slightly reduced performance but dramatically smaller size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0FfKUYdaOJj"
      },
      "source": [
        "### 6.1 Visualizing the speed vs. performance tradeoff\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmdH2AS_aOJj"
      },
      "source": [
        "\n",
        "We've seen that our ViT model outperforms our EffNetB2 model in terms of performance metrics such as test loss and test accuracy.\n",
        "\n",
        "However, our EffNetB2 model performs predictions faster and has a much smaller model size.\n",
        "\n",
        "> **Note:** Performance or inference time is also often referred to as \"latency\".\n",
        "\n",
        "How about we make this fact visual?\n",
        "\n",
        "We can do so by creating a plot with matplotlib:\n",
        "1. Create a scatter plot from the comparison DataFrame to compare EffNetB2 and ViT `time_per_pred_cpu` and `test_acc` values.\n",
        "2. Add titles and labels respective of the data and customize the fontsize for aesthetics.\n",
        "3. Annotate the samples on the scatter plot from step 1 with their appropriate labels (the model names).\n",
        "4. Create a legend based on the model sizes (`model_size (MB)`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "3plB-RzgaOJj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "outputId": "fe53b666-45d1-446c-af28-a3faf014ec9a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 1. Create a plot from model comparison DataFrame\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "scatter = ax.scatter(data=df,\n",
        "                     x=\"time_per_pred_cpu\",\n",
        "                     y=\"test_acc\",\n",
        "                     c=[\"blue\", \"orange\"], # what colours to use?\n",
        "                     s=\"model_size (MB)\") # size the dots by the model sizes\n",
        "\n",
        "# 2. Add titles, labels and customize fontsize for aesthetics\n",
        "ax.set_title(\"FoodVision Mini Inference Speed vs Performance\", fontsize=18)\n",
        "ax.set_xlabel(\"Prediction time per image (seconds)\", fontsize=14)\n",
        "ax.set_ylabel(\"Test accuracy (%)\", fontsize=14)\n",
        "ax.tick_params(axis='both', labelsize=12)\n",
        "ax.grid(True)\n",
        "\n",
        "# 3. Annotate with model names\n",
        "for index, row in df.iterrows():\n",
        "    ax.annotate(text=row[\"model\"], # note: depending on your version of Matplotlib, you may need to use \"s=...\" or \"text=...\", see: https://github.com/faustomorales/keras-ocr/issues/183#issuecomment-977733270\n",
        "                xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03),\n",
        "                size=12)\n",
        "\n",
        "# 4. Create a legend based on model sizes\n",
        "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5)\n",
        "model_size_legend = ax.legend(handles,\n",
        "                              labels,\n",
        "                              loc=\"upper right\",\n",
        "                              title=\"Model size (MB)\",\n",
        "                              fontsize=12)\n",
        "\n",
        "# Save the figure\n",
        "!mkdir images/\n",
        "plt.savefig(\"images/09-foodvision-mini-inference-speed-vs-performance.jpg\")\n",
        "\n",
        "# Show the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx_-vtVFaOJj"
      },
      "source": [
        "Woah!\n",
        "\n",
        "The plot really visualizes the **speed vs. performance tradeoff**, in other words, when you have a larger, better performing deep model (like our ViT model), it *generally* takes longer to perform inference (higher latency).\n",
        "\n",
        "There are exceptions to the rule and new research is being published all the time to help make larger models perform faster.\n",
        "\n",
        "And it can be tempting to just deploy the *best* performing model but it's also good to take into consideration where the model is going to be performing.\n",
        "\n",
        "In our case, the differences between our model's performance levels (on the test loss and test accuracy) aren't too extreme.\n",
        "\n",
        "But since we'd like to put an emphasis on speed to begin with, we're going to stick with deploying EffNetB2 since it's faster and has a much smaller footprint.\n",
        "\n",
        "> **Note:** Prediction times will be different across different hardware types (e.g. Intel i9 vs Google Colab CPU vs GPU) so it's important to think about and test where your model is going to end up. Asking questions like \"where is the model going to be run?\" or \"what is the ideal scenario for running the model?\" and then running experiments to try and provide answers on your way to deployment is very helpful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ6mqRataOJj"
      },
      "source": [
        "## 7. Bringing FoodVision Mini to life by creating a Gradio demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GvkjXoOaOJj"
      },
      "source": [
        "\n",
        "We've decided we'd like to deploy the EffNetB2 model (to begin with, this could always be changed later).\n",
        "\n",
        "So how can we do that?\n",
        "\n",
        "There are several ways to deploy a machine learning model each with specific use cases (as discussed above).\n",
        "\n",
        "We're going to be focused on perhaps the quickest and certainly one of the most fun ways to get a model deployed to the internet.\n",
        "\n",
        "And that's by using [Gradio](https://gradio.app/).\n",
        "\n",
        "What's Gradio?\n",
        "\n",
        "The homepage describes it beautifully:\n",
        "\n",
        "> Gradio is the fastest way to demo your machine learning model with a friendly web interface so that anyone can use it, anywhere!\n",
        "\n",
        "Why create a demo of your models?\n",
        "\n",
        "Because metrics on the test set look nice but you never really know how your model performs until you use it in the wild.\n",
        "\n",
        "So let's get deploying!\n",
        "\n",
        "We'll start by importing Gradio with the common alias `gr` and if it's not present, we'll install it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "hIRnLPogaOJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "053bb70e-ec97-4c00-8e6c-f1367f4891f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.3/51.3 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hGradio version: 5.23.0\n"
          ]
        }
      ],
      "source": [
        "# Import/install Gradio\n",
        "try:\n",
        "    import gradio as gr\n",
        "except:\n",
        "    !pip -q install gradio\n",
        "    import gradio as gr\n",
        "\n",
        "print(f\"Gradio version: {gr.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvdEIyUoaOJj"
      },
      "source": [
        "Gradio ready!\n",
        "\n",
        "Let's turn FoodVision Mini into a demo application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ2cxO-RaOJj"
      },
      "source": [
        "### 7.1 Gradio overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSSbbNxjaOJj"
      },
      "source": [
        "\n",
        "The overall premise of Gradio is very similar to what we've been repeating throughout the course.\n",
        "\n",
        "What are our **inputs** and **outputs**?\n",
        "\n",
        "And how should we get there?\n",
        "\n",
        "Well that's what our machine learning model does.\n",
        "\n",
        "```\n",
        "inputs -> ML model -> outputs\n",
        "```\n",
        "\n",
        "In our case, for FoodVision Mini, our inputs are images of food, our ML model is EffNetB2 and our outputs are classes of food (pizza, steak or sushi).\n",
        "\n",
        "```\n",
        "images of food -> EffNetB2 -> outputs\n",
        "```\n",
        "\n",
        "Though the concepts of inputs and outputs can be bridged to almost any other kind of ML problem.\n",
        "\n",
        "Your inputs and outputs might be any combination of the following:\n",
        "* Images\n",
        "* Text\n",
        "* Video\n",
        "* Tabular data\n",
        "* Audio\n",
        "* Numbers\n",
        "* & more\n",
        "\n",
        "And the ML model you build will depend on your inputs and outputs.\n",
        "\n",
        "Gradio emulates this paradigm by creating an interface ([`gradio.Interface()`](https://gradio.app/docs/#interface-header)) from inputs to outputs.\n",
        "\n",
        "```\n",
        "gradio.Interface(fn, inputs, outputs)\n",
        "```\n",
        "\n",
        "Where, `fn` is a Python function to map the `inputs` to the `outputs`.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-gradio-workflow.png\" alt=\"gradio workflow of inputs flowing into some kind of model or function and then producing outputs\" width=900/>\n",
        "\n",
        "*Gradio provides a very helpful `Interface` class to easily create an inputs -> model/function -> outputs workflow where the inputs and outputs could be almost anything you want. For example, you might input Tweets (text) to see if they're about machine learning or not or [input a text prompt to generate images](https://huggingface.co/blog/stable_diffusion).*\n",
        "\n",
        "> **Note:** Gradio has a vast number of possible `inputs` and `outputs` options known as \"Components\" from images to text to numbers to audio to videos and more. You can see all of these in the [Gradio Components documentation](https://gradio.app/docs/#components)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl3b69NBaOJj"
      },
      "source": [
        "### 7.2 Creating a function to map our inputs and outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w26P_z_FaOJj"
      },
      "source": [
        "\n",
        "To create our FoodVision Mini demo with Gradio, we'll need a function to map our inputs to our outputs.\n",
        "\n",
        "We created a function earlier called `pred_and_store()` to make predictions with a given model across a list of target files and store them in a list of dictionaries.\n",
        "\n",
        "How about we create a similar function but this time focusing on making a prediction on a single image with our EffNetB2 model?\n",
        "\n",
        "More specifically, we want a function that takes an image as input, preprocesses (transforms) it, makes a prediction with EffNetB2 and then returns the prediction (pred or pred label for short) as well as the prediction probability (pred prob).\n",
        "\n",
        "And while we're here, let's return the time it took to do so too:\n",
        "\n",
        "```\n",
        "input: image -> transform -> predict with EffNetB2 -> output: pred, pred prob, time taken\n",
        "```\n",
        "\n",
        "This will be our `fn` parameter for our Gradio interface.\n",
        "\n",
        "First, let's make sure our EffNetB2 model is on the CPU (since we're sticking with CPU-only predictions, however you could change this if you have access to a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "fe6dRYa0aOJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0fa0ddc-71cb-4bf1-b6df-31f6c7ca4897"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# Put EffNetB2 on CPU\n",
        "effnetb2.to(\"cpu\")\n",
        "\n",
        "# Check the device\n",
        "next(iter(effnetb2.parameters())).device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN25ah0BaOJj"
      },
      "source": [
        "And now let's create a function called `predict()` to replicate the workflow above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "8LhhWI9uaOJj"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Dict\n",
        "\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
        "    \"\"\"\n",
        "    # Start the timer\n",
        "    start_time = timer()\n",
        "\n",
        "    # Transform the target image and add a batch dimension\n",
        "    img = effnetb2_transforms(img).unsqueeze(0)\n",
        "\n",
        "    # Put model into evaluation mode and turn on inference mode\n",
        "    effnetb2.eval()\n",
        "    with torch.inference_mode():\n",
        "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
        "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
        "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "    # Calculate the prediction time\n",
        "    pred_time = round(timer() - start_time, 5)\n",
        "\n",
        "    # Return the prediction dictionary and prediction time\n",
        "    return pred_labels_and_probs, pred_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk4kfzReaOJk"
      },
      "source": [
        "**Code Explanation**  \n",
        "\n",
        "```python\n",
        "from typing import Tuple, Dict  \n",
        "\n",
        "def predict(img) -> Tuple[Dict, float]:  \n",
        "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.  \n",
        "    \"\"\"  \n",
        "```  \n",
        "\n",
        "- **Function Purpose**:  \n",
        "  - Takes an image (`img`) as input and returns:  \n",
        "    - A dictionary of predicted class probabilities (`Dict`).  \n",
        "    - Inference time (`float`) in seconds.  \n",
        "\n",
        "\n",
        "**1. Timer Setup**  \n",
        "- ```python 09_pytorch_model_deployment_notes.ipynb  \n",
        "  # Start the timer  \n",
        "  start_time = timer()  \n",
        "  ```  \n",
        "    - Records the start time for measuring inference speed.  \n",
        "\n",
        "\n",
        "**2. Image Transformation**  \n",
        "- ```python 09_pytorch_model_deployment_notes.ipynb  \n",
        "  # Transform the target image and add a batch dimension  \n",
        "  img = effnetb2_transforms(img).unsqueeze(0)  \n",
        "  ```  \n",
        "    - Applies `effnetb2_transforms` (e.g., resizing, normalization) to `img`.  \n",
        "    - `unsqueeze(0)` adds a batch dimension (shape `[1, C, H, W]`) for model input.  \n",
        "\n",
        "\n",
        "\n",
        "**3. Model Inference**  \n",
        "- ```python 09_pytorch_model_deployment_notes.ipynb  \n",
        "  # Put model into evaluation mode and turn on inference mode  \n",
        "  effnetb2.eval()  \n",
        "  with torch.inference_mode():  \n",
        "      # Pass the transformed image through the model  \n",
        "      pred_probs = torch.softmax(effnetb2(img), dim=1)  \n",
        "  ```  \n",
        "    - `eval()`: Disables dropout/batch norm for inference.  \n",
        "    - `inference_mode()`: Optimizes inference (no gradient tracking).  \n",
        "    - `torch.softmax(..., dim=1)`: Converts logits to probabilities (sums to 1).  \n",
        "\n",
        "\n",
        "**4. Prediction Dictionary Creation**  \n",
        "- ```python 09_pytorch_model_deployment_notes.ipynb  \n",
        "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}  \n",
        "  ```  \n",
        "  - **Syntax Breakdown**:  \n",
        "    - `class_names[i]`: Accesses the class name (string) at index `i`.  \n",
        "      - Assumes `class_names` is a list like `[\"cat\", \"dog\"]`.  \n",
        "    - `:`: Separates keys (class names) and values (probabilities) in the dictionary.  \n",
        "    - `pred_probs[0][i]`:  \n",
        "      - `pred_probs` is a tensor of shape `[1, num_classes]` (batch of 1).  \n",
        "      - `[0]` selects the first (only) batch item.  \n",
        "      - `[i]` gets the probability for class `i`.  \n",
        "    - `for i in range(len(class_names))`:  \n",
        "      - Loops over all class indices (e.g., 0 to 1 for 2 classes).  \n",
        "      - Ensures each class name maps to its probability.  \n",
        "\n",
        "\n",
        "\n",
        "**5. Time Calculation**  \n",
        "- ```python 09_pytorch_model_deployment_notes.ipynb  \n",
        "  # Calculate the prediction time  \n",
        "  pred_time = round(timer() - start_time, 5)  \n",
        "  ```  \n",
        "    - Computes elapsed time and rounds to 5 decimal places.  \n",
        "\n",
        "\n",
        "\n",
        "**6. Return Values**  \n",
        "- ```python 09_pytorch_model_deployment_notes.ipynb  \n",
        "  return pred_labels_and_probs, pred_time  \n",
        "  ```  \n",
        "  - Returns:  \n",
        "    - `pred_labels_and_probs`: Dictionary like `{\"cat\": 0.9, \"dog\": 0.1}`.  \n",
        "    - `pred_time`: Inference time (e.g., `0.04521` seconds).  \n",
        "\n",
        "\n",
        "\n",
        "**Key Notes**  \n",
        "- **Input/Output**:  \n",
        "  - Input: Raw image (PIL or tensor).  \n",
        "  - Output: Human-readable predictions + timing.  \n",
        "- **Assumptions**:  \n",
        "  - `effnetb2` and `effnetb2_transforms` are predefined.  \n",
        "  - `class_names` is a list of class labels.  \n",
        "- **Performance**: Optimized for inference (no gradients, batch processing).  \n",
        "\n",
        "Let me know if you'd like further clarification!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r22gASUaOJk"
      },
      "source": [
        "Beautiful!\n",
        "\n",
        "Now let's see our function in action by performing a prediction on a random image from the test dataset.\n",
        "\n",
        "We'll start by getting a list of all the image paths from the test directory and then randomly selecting one.\n",
        "\n",
        "Then we'll open the randomly selected image with [`PIL.Image.open()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#functions).\n",
        "\n",
        "Finally, we'll pass the image to our `predict()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "rxwTlt2PaOJk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f0e52f-02df-45d2-b900-10d329b96c8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Predicting on image at path: data/pizza_steak_sushi_20_percent/test/sushi/715227.jpg\n",
            "\n",
            "Prediction label and probability dictionary: \n",
            "{'pizza': 0.17079780995845795, 'steak': 0.09334960579872131, 'sushi': 0.7358525991439819}\n",
            "Prediction time: 0.09276 seconds\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Get a list of all test image filepaths\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "\n",
        "# Randomly select a test image path\n",
        "random_image_path = random.sample(test_data_paths, k=1)[0]\n",
        "\n",
        "# Open the target image\n",
        "image = Image.open(random_image_path)\n",
        "print(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n",
        "\n",
        "# Predict on the target image and print out the outputs\n",
        "pred_dict, pred_time = predict(img=image)\n",
        "print(f\"Prediction label and probability dictionary: \\n{pred_dict}\")\n",
        "print(f\"Prediction time: {pred_time} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdB7l98KaOJk"
      },
      "source": [
        "Nice!\n",
        "\n",
        "Running the cell above a few times we can see different prediction probabilities for each label from our EffNetB2 model as well as the time it took per prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUYYCuoMaOJk"
      },
      "source": [
        "### 7.3 Creating a list of example images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVq-Kp-_aOJk"
      },
      "source": [
        "\n",
        "Our `predict()` function enables us to go from inputs -> transform -> ML model -> outputs.\n",
        "\n",
        "Which is exactly what we need for our Graido demo.\n",
        "\n",
        "But before we create the demo, let's create one more thing: a list of examples.\n",
        "\n",
        "Gradio's [`Interface`](https://gradio.app/docs/#interface) class takes a list of `examples` of as an optional parameter (`gradio.Interface(examples=List[Any])`).\n",
        "\n",
        "And the format for the `examples` parameter is a list of lists.\n",
        "\n",
        "So let's create a list of lists containing random filepaths to our test images.\n",
        "\n",
        "Three examples should be enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "StyH03gJaOJk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c5346a4-06f2-4a7c-adfb-aeba98b952a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['data/pizza_steak_sushi_20_percent/test/sushi/1203702.jpg'],\n",
              " ['data/pizza_steak_sushi_20_percent/test/sushi/3365273.jpg'],\n",
              " ['data/pizza_steak_sushi_20_percent/test/steak/3757027.jpg']]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# Create a list of example inputs to our Gradio demo\n",
        "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\n",
        "example_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipeSY2jvaOJk"
      },
      "source": [
        "Perfect!\n",
        "\n",
        "Our Gradio demo will showcase these as example inputs to our demo so people can try it out and see what it does without uploading any of their own data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dylw93j3aOJk"
      },
      "source": [
        "### 7.4 Building a Gradio interface\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLsWskKIaOJk"
      },
      "source": [
        "\n",
        "Time to put everything together and bring our FoodVision Mini demo to life!\n",
        "\n",
        "Let's create a Gradio interface to replicate the workflow:\n",
        "\n",
        "```\n",
        "input: image -> transform -> predict with EffNetB2 -> output: pred, pred prob, time taken\n",
        "```\n",
        "\n",
        "We can do with the [`gradio.Interface()`](https://gradio.app/docs/#interface) class with the following parameters:\n",
        "* `fn` - a Python function to map `inputs` to `outputs`, in our case, we'll use our `predict()` function.\n",
        "* `inputs` - the input to our interface, such as an image using [`gradio.Image()`](https://gradio.app/docs/#image) or `\"image\"`.\n",
        "* `outputs` - the output of our interface once the `inputs` have gone through the `fn`, such as a label using [`gradio.Label()`](https://gradio.app/docs/#label) (for our model's predicted labels) or number using [`gradio.Number()`](https://gradio.app/docs/#number) (for our model's prediction time).\n",
        "    * **Note:** Gradio comes with many in-built `inputs` and `outputs` options known as [\"Components\"](https://gradio.app/docs/#components).\n",
        "* `examples` - a list of examples to showcase for the demo.\n",
        "* `title` - a string title of the demo.\n",
        "* `description` - a string description of the demo.\n",
        "* `article` - a reference note at the bottom of the demo.\n",
        "\n",
        "Once we've created our demo instance of `gr.Interface()`, we can bring it to life using [`gradio.Interface().launch()`](https://gradio.app/docs/#launch-header) or `demo.launch()` command.\n",
        "\n",
        "Easy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ZKgltFV9aOJk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "b3ac26dc-03ea-4cfc-b503-01eb63a2c452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://36d8960627f73d0d78.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://36d8960627f73d0d78.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create title, description and article strings\n",
        "title = \"FoodVision Mini üçïü•©üç£\"\n",
        "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
        "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=predict, # mapping function from input to output\n",
        "                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n",
        "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n",
        "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch(debug=False, # print errors locally?\n",
        "            share=True) # generate a publically shareable URL?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUkcfmhqaOJk"
      },
      "source": [
        "<img src=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/09-gradio-running-in-google-colab-and-in-browser.gif\" alt=\"Gradio demo running in Google Colab and on the web\" width=750/>\n",
        "\n",
        "*FoodVision Mini Gradio demo running in Google Colab and in the browser (the link when running from Google Colab only lasts for 72 hours). You can see the [permanent live demo on Hugging Face Spaces](https://huggingface.co/spaces/mrdbourke/foodvision_mini).*\n",
        "\n",
        "Woohoo!!! What an epic demo!!!\n",
        "\n",
        "FoodVision Mini has officially come to life in an interface someone could use and try out.\n",
        "\n",
        "If you set the parameter `share=True` in the `launch()` method, Gradio also provides you with a shareable link such as `https://123XYZ.gradio.app` (this link is an example only and likely expired) which is valid for 72-hours.\n",
        "\n",
        "The link provides a proxy back to the Gradio interface you launched.\n",
        "\n",
        "For more permanent hosting, you can upload your Gradio app to [Hugging Face Spaces](https://huggingface.co/spaces) or anywhere that runs Python code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4FE5qdBaOJk"
      },
      "source": [
        "## 8. Turning our FoodVision Mini Gradio Demo into a deployable app\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty6eTRvtaOJk"
      },
      "source": [
        "\n",
        "We've seen our FoodVision Mini model come to life through a Gradio demo.\n",
        "\n",
        "But what if we wanted to share it with our friends?\n",
        "\n",
        "Well, we could use the provided Gradio link, however, the shared link only lasts for 72-hours.\n",
        "\n",
        "To make our FoodVision Mini demo more permanent, we can package it into an app and upload it to [Hugging Face Spaces](https://huggingface.co/spaces/launch)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YJK5zBvaOJk"
      },
      "source": [
        "### 8.1 What is Hugging Face Spaces?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg57YCCqaOJk"
      },
      "source": [
        "\n",
        "Hugging Face Spaces is a resource that allows you to host and share machine learning apps.\n",
        "\n",
        "Building a demo is one of the best ways to showcase and test what you've done.\n",
        "\n",
        "And Spaces allows you to do just that.\n",
        "\n",
        "You can think of Hugging Face as the GitHub of machine learning.\n",
        "\n",
        "If having a good GitHub portfolio showcases your coding abilities, having a good Hugging Face portfolio can showcase your machine learning abilities.\n",
        "\n",
        "> **Note:** There are many other places we could upload and host our Gradio app such as, Google Cloud, AWS (Amazon Web Services) or other cloud vendors, however, we're going to use Hugging Face Spaces due to the ease of use and wide adoption by the machine learning community."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JQX5OLqaOJl"
      },
      "source": [
        "### 8.2 Deployed Gradio app structure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue4OAApEaOJl"
      },
      "source": [
        "\n",
        "To upload our demo Gradio app, we'll want to put everything relating to it into a single directory.\n",
        "\n",
        "For example, our demo might live at the path `demos/foodvision_mini/` with the file structure:\n",
        "\n",
        "```\n",
        "demos/\n",
        "‚îî‚îÄ‚îÄ foodvision_mini/\n",
        "    ‚îú‚îÄ‚îÄ 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n",
        "    ‚îú‚îÄ‚îÄ app.py\n",
        "    ‚îú‚îÄ‚îÄ examples/\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ example_1.jpg\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ example_2.jpg\n",
        "    ‚îÇ   ‚îî‚îÄ‚îÄ example_3.jpg\n",
        "    ‚îú‚îÄ‚îÄ model.py\n",
        "    ‚îî‚îÄ‚îÄ requirements.txt\n",
        "```\n",
        "\n",
        "Where:\n",
        "* `09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth` is our trained PyTorch model file.\n",
        "* `app.py` contains our Gradio app (similar to the code that launched the app).\n",
        "    * **Note:** `app.py` is the default filename used for Hugging Face Spaces, if you deploy your app there, Spaces will by default look for a file called `app.py` to run. This is changeable in settings.\n",
        "* `examples/` contains example images to use with our Gradio app.\n",
        "* `model.py` contains the model definition as well as any transforms associated with the model.\n",
        "* `requirements.txt` contains the dependencies to run our app such as `torch`, `torchvision` and `gradio`.\n",
        "\n",
        "Why this way?\n",
        "\n",
        "Because it's one of the simplest layouts we could begin with.\n",
        "\n",
        "Our focus is: *experiment, experiment, experiment!*\n",
        "\n",
        "The quicker we can run smaller experiments, the better our bigger ones will be.\n",
        "\n",
        "We're going to work towards recreating the structure above but you can see a live demo app running on Hugging Face Spaces as well as the file structure:\n",
        "* [Live Gradio demo of FoodVision Mini üçïü•©üç£](https://huggingface.co/spaces/mrdbourke/foodvision_mini).\n",
        "* [FoodVision Mini file structure on Hugging Face Spaces](https://huggingface.co/spaces/mrdbourke/foodvision_mini/tree/main)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_CvNNFJaOJl"
      },
      "source": [
        "### 8.3 Creating a `demos` folder to store our FoodVision Mini app files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49x35QMXaOJl"
      },
      "source": [
        "\n",
        "To begin, let's first create a `demos/` directory to store all of our FoodVision Mini app files.\n",
        "\n",
        "We can do with Python's [`pathlib.Path(\"path_to_dir\")`](https://docs.python.org/3/library/pathlib.html#basic-use) to establish the directory path and [`pathlib.Path(\"path_to_dir\").mkdir()`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir) to create it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Zzec64LtaOJl"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Create FoodVision mini demo path\n",
        "foodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")\n",
        "\n",
        "# Remove files that might already exist there and create new directory\n",
        "if foodvision_mini_demo_path.exists():\n",
        "    shutil.rmtree(foodvision_mini_demo_path)\n",
        "# If the file doesn't exist, create it anyway\n",
        "foodvision_mini_demo_path.mkdir(parents=True,\n",
        "                                exist_ok=True)\n",
        "\n",
        "# Check what's in the folder\n",
        "!ls demos/foodvision_mini/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7MA7pHJaOJl"
      },
      "source": [
        "### 8.4 Creating a folder of example images to use with our FoodVision Mini demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2e0KNMLaOJl"
      },
      "source": [
        "\n",
        "Now we've got a directory to store our FoodVision Mini demo files, let's add some examples to it.\n",
        "\n",
        "Three example images from the test dataset should be enough.\n",
        "\n",
        "To do so we'll:\n",
        "1. Create an `examples/` directory within the `demos/foodvision_mini` directory.\n",
        "2. Choose three random images from the test dataset and collect their filepaths in a list.\n",
        "3. Copy the three random images from the test dataset to the `demos/foodvision_mini/examples/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "_7mMJGD5aOJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd05e10-fdeb-4bd6-ea99-263e75fd8f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Copying data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg to demos/foodvision_mini/examples/592799.jpg\n",
            "[INFO] Copying data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg to demos/foodvision_mini/examples/3622237.jpg\n",
            "[INFO] Copying data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg to demos/foodvision_mini/examples/2582289.jpg\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Create an examples directory\n",
        "foodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\"\n",
        "foodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Collect three random test dataset image paths\n",
        "foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n",
        "                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n",
        "                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n",
        "\n",
        "# 3. Copy the three random images to the examples directory\n",
        "for example in foodvision_mini_examples:\n",
        "    destination = foodvision_mini_examples_path / example.name\n",
        "    print(f\"[INFO] Copying {example} to {destination}\")\n",
        "    shutil.copy2(src=example, dst=destination)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoGCb0C8aOJl"
      },
      "source": [
        "Now to verify our examples are present, let's list the contents of our `demos/foodvision_mini/examples/` directory with [`os.listdir()`](https://docs.python.org/3/library/os.html#os.listdir) and then format the filepaths into a list of lists (so it's compatible with Gradio's [`gradio.Interface()`](https://gradio.app/docs/#interface) `example` parameter)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "VCJ0GUkPaOJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bd37e85-e094-4b39-ceb1-e7e9b02ed870"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['examples/2582289.jpg'], ['examples/592799.jpg'], ['examples/3622237.jpg']]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Get example filepaths in a list of lists\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\n",
        "example_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooqZJLi6aOJl"
      },
      "source": [
        "### 8.5 Moving our trained EffNetB2 model to our FoodVision Mini demo directory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32GS0fAqaOJl"
      },
      "source": [
        "\n",
        "We previously saved our FoodVision Mini EffNetB2 feature extractor model under `models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth`.\n",
        "\n",
        "And rather double up on saved model files, let's move our model to our `demos/foodvision_mini` directory.\n",
        "\n",
        "We can do so using Python's [`shutil.move()`](https://docs.python.org/3/library/shutil.html#shutil.move) method and passing in `src` (the source path of the target file) and `dst` (the destination path of the target file to be moved to) parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "tul7mBSdaOJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc8375e-b6a6-4891-9a5f-bb7a1c689538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Attempting to move models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth to demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n",
            "[INFO] Model move complete.\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# Create a source path for our target model\n",
        "effnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n",
        "\n",
        "# Create a destination path for our target model\n",
        "effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n",
        "\n",
        "# Try to move the file\n",
        "try:\n",
        "    print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n",
        "\n",
        "    # Move the model\n",
        "    shutil.move(src=effnetb2_foodvision_mini_model_path,\n",
        "                dst=effnetb2_foodvision_mini_model_destination)\n",
        "\n",
        "    print(f\"[INFO] Model move complete.\")\n",
        "\n",
        "# If the model has already been moved, check if it exists\n",
        "except:\n",
        "    print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")\n",
        "    print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF0xssS0aOJl"
      },
      "source": [
        "### 8.6 Turning our EffNetB2 model into a Python script (`model.py`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUyIeBT1aOJl"
      },
      "source": [
        "\n",
        "Our current model's `state_dict` is saved to `demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth`.\n",
        "\n",
        "To load it in we can use `model.load_state_dict()` along with `torch.load()`.\n",
        "\n",
        "> **Note:** For a refresh on saving and loading a model (or a model's `state_dict` in PyTorch, see [01. PyTorch Workflow Fundamentals section 5: Saving and loading a PyTorch model](https://www.learnpytorch.io/01_pytorch_workflow/#5-saving-and-loading-a-pytorch-model) or see the PyTorch recipe for [What is a `state_dict` in PyTorch?](https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html)\n",
        "\n",
        "But before we can do this, we first need a way to instantiate a `model`.\n",
        "\n",
        "To do this in a modular fashion we'll create a script called `model.py` which contains our `create_effnetb2_model()` function we created in [section 3.1: *Creating a function to make an EffNetB2 feature extractor*](https://www.learnpytorch.io/09_pytorch_model_deployment/#31-creating-a-function-to-make-an-effnetb2-feature-extractor).\n",
        "\n",
        "That way we can import the function in *another* script (see `app.py` below) and then use it to create our EffNetB2 `model` instance as well as get its appropriate transforms.\n",
        "\n",
        "Just like in [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/), we'll use the `%%writefile path/to/file` magic command to turn a cell of code into a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "E2eO22CSaOJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d93bb3fe-3ba4-4d55-d955-e8564d5a6b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_mini/model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile demos/foodvision_mini/model.py\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def create_effnetb2_model(num_classes:int=3,\n",
        "                          seed:int=42):\n",
        "    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int, optional): number of classes in the classifier head.\n",
        "            Defaults to 3.\n",
        "        seed (int, optional): random seed value. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "        model (torch.nn.Module): EffNetB2 feature extractor model.\n",
        "        transforms (torchvision.transforms): EffNetB2 image transforms.\n",
        "    \"\"\"\n",
        "    # Create EffNetB2 pretrained weights, transforms and model\n",
        "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "    # Freeze all layers in base model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Change classifier head with random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3, inplace=True),\n",
        "        nn.Linear(in_features=1408, out_features=num_classes),\n",
        "    )\n",
        "\n",
        "    return model, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81tgyyG2aOJl"
      },
      "source": [
        "### 8.7 Turning our FoodVision Mini Gradio app into a Python script (`app.py`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFbzYt2qaOJl"
      },
      "source": [
        "\n",
        "We've now got a `model.py` script as well as a path to a saved model `state_dict` that we can load in.\n",
        "\n",
        "Time to construct `app.py`.\n",
        "\n",
        "We call it `app.py` because by default when you create a HuggingFace Space, it looks for a file called `app.py` to run and host (though you can change this in settings).\n",
        "\n",
        "Our `app.py` script will put together all of the pieces of the puzzle to create our Gradio demo and will have four main parts:\n",
        "\n",
        "1. **Imports and class names setup** - Here we'll import the various dependencies for our demo including the `create_effnetb2_model()` function from `model.py` as well as setup the different class names for our FoodVision Mini app.\n",
        "2. **Model and transforms preparation** - Here we'll create an EffNetB2 model instance along with the transforms to go with it and then we'll load in the saved model weights/`state_dict`. When we load the model we'll also set `map_location=torch.device(\"cpu\")` in [`torch.load()`](https://pytorch.org/docs/stable/generated/torch.load.html) so our model gets loaded onto the CPU regardless of the device it trained on (we do this because we won't necessarily have a GPU when we deploy and we'll get an error if our model is trained on GPU but we try to deploy it to CPU without explicitly saying so).\n",
        "3. **Predict function** - Gradio's `gradio.Interface()` takes a `fn` parameter to map inputs to outputs, our `predict()` function will be the same as the one we defined above in [section 7.2: *Creating a function to map our inputs and outputs*](https://www.learnpytorch.io/09_pytorch_model_deployment/#72-creating-a-function-to-map-our-inputs-and-outputs), it will take in an image and then use the loaded transforms to preprocess it before using the loaded model to make a prediction on it.\n",
        "    * **Note:** We'll have to create the example list on the fly via the `examples` parameter. We can do so by creating a list of the files inside the `examples/` directory with: `[[\"examples/\" + example] for example in os.listdir(\"examples\")]`.\n",
        "4. **Gradio app** - This is where the main logic of our demo will live, we'll create a `gradio.Interface()` instance called `demo` to put together our inputs, `predict()` function and outputs. And we'll finish the script by calling `demo.launch()` to launch our FoodVision Mini demo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "C2uPgrKVaOJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ccd455-7632-4d4c-8655-3599b81b117c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_mini/app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile demos/foodvision_mini/app.py\n",
        "### 1. Imports and class names setup ###\n",
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from model import create_effnetb2_model\n",
        "from timeit import default_timer as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Setup class names\n",
        "class_names = [\"pizza\", \"steak\", \"sushi\"]\n",
        "\n",
        "### 2. Model and transforms preparation ###\n",
        "\n",
        "# Create EffNetB2 model\n",
        "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
        "    num_classes=3, # len(class_names) would also work\n",
        ")\n",
        "\n",
        "# Load saved weights\n",
        "effnetb2.load_state_dict(\n",
        "    torch.load(\n",
        "        f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n",
        "        map_location=torch.device(\"cpu\"),  # load to CPU\n",
        "    )\n",
        ")\n",
        "\n",
        "### 3. Predict function ###\n",
        "\n",
        "# Create predict function\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
        "    \"\"\"\n",
        "    # Start the timer\n",
        "    start_time = timer()\n",
        "\n",
        "    # Transform the target image and add a batch dimension\n",
        "    img = effnetb2_transforms(img).unsqueeze(0)\n",
        "\n",
        "    # Put model into evaluation mode and turn on inference mode\n",
        "    effnetb2.eval()\n",
        "    with torch.inference_mode():\n",
        "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
        "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
        "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "    # Calculate the prediction time\n",
        "    pred_time = round(timer() - start_time, 5)\n",
        "\n",
        "    # Return the prediction dictionary and prediction time\n",
        "    return pred_labels_and_probs, pred_time\n",
        "\n",
        "### 4. Gradio app ###\n",
        "\n",
        "# Create title, description and article strings\n",
        "title = \"FoodVision Mini üçïü•©üç£\"\n",
        "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\n",
        "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
        "\n",
        "# Create examples list from \"examples/\" directory\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=predict, # mapping function from input to output\n",
        "                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n",
        "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n",
        "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
        "                    # Create examples list from \"examples/\" directory\n",
        "                    examples=example_list,\n",
        "                    title=title,\n",
        "                    description=description,\n",
        "                    article=article)\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk3I9A9daOJm"
      },
      "source": [
        "### 8.8 Creating a requirements file for FoodVision Mini (`requirements.txt`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjqhaoOCaOJm"
      },
      "source": [
        "\n",
        "The last file we need to create for our FoodVision Mini app is a [`requirements.txt` file](https://learnpython.com/blog/python-requirements-file/).\n",
        "\n",
        "This will be a text file containing all of the required dependencies for our demo.\n",
        "\n",
        "When we deploy our demo app to Hugging Face Spaces, it will search through this file and install the dependencies we define so our app can run.\n",
        "\n",
        "The good news is, there's only three!\n",
        "\n",
        "1. `torch==1.12.0`\n",
        "2. `torchvision==0.13.0`\n",
        "3. `gradio==3.1.4`\n",
        "\n",
        "The \"`==1.12.0`\" states the version number to install.\n",
        "\n",
        "Defining the version number is not 100% required but we will for now so if any breaking updates occur in future releases, our app still runs (PS if you find any errors, feel free to post on the course [GitHub Issues](https://github.com/mrdbourke/pytorch-deep-learning/issues))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "KOQ5WWbUaOJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ae8570-8463-4f5c-8574-2dc411ed6849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_mini/requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile demos/foodvision_mini/requirements.txt\n",
        "torch==1.12.0\n",
        "torchvision==0.13.0\n",
        "gradio==3.1.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2TVkZrtaOJm"
      },
      "source": [
        "Nice!\n",
        "\n",
        "We've officially got all the files we need to deploy our FoodVision Mini demo!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88b7433c7dd2483f83048aaccf7399cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41a7921a7c7e4400abc1fd006005ff0a",
              "IPY_MODEL_4a714878c8a84d968cc72402ebffafd9",
              "IPY_MODEL_04b6054cd0d5462ea2cd72a83d0aa011"
            ],
            "layout": "IPY_MODEL_2b9a4fdbc05f4a06a9506b289fe003ac"
          }
        },
        "41a7921a7c7e4400abc1fd006005ff0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ef0bba27433481ab4e3d9abba6dbcef",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1163d11a4a484e8fba1f12422caa9bc2",
            "value": "100%"
          }
        },
        "4a714878c8a84d968cc72402ebffafd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3784e0fad1a748bc9eb00a80f5a807c3",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5a2470875ca4dd194492ca79240a23a",
            "value": 10
          }
        },
        "04b6054cd0d5462ea2cd72a83d0aa011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db903a61101441c382b78132f799e622",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f315054b14fc4968b73d46d030119d38",
            "value": "‚Äá10/10‚Äá[01:01&lt;00:00,‚Äá‚Äá5.73s/it]"
          }
        },
        "2b9a4fdbc05f4a06a9506b289fe003ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ef0bba27433481ab4e3d9abba6dbcef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1163d11a4a484e8fba1f12422caa9bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3784e0fad1a748bc9eb00a80f5a807c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5a2470875ca4dd194492ca79240a23a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db903a61101441c382b78132f799e622": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f315054b14fc4968b73d46d030119d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "179c5bf44f0047daa241675ccc4af03b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b43c0866d24449eb345d3e92b3eb954",
              "IPY_MODEL_bb3cf6ebc2b84eba95080d7b6efed8ec",
              "IPY_MODEL_bc0c907b890345fb8e25c30e91adcfdf"
            ],
            "layout": "IPY_MODEL_93ad48eb2d714e9790500cd7cd749fad"
          }
        },
        "6b43c0866d24449eb345d3e92b3eb954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e21fe6af4474428a91637bcfc258312",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ca07bb3ee20a4afeb554b5bcdbfc1b6b",
            "value": "100%"
          }
        },
        "bb3cf6ebc2b84eba95080d7b6efed8ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfea2a09502a4db3acda9c19360ab8ad",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_392ad330efb14c419a641c1bc8f43cc1",
            "value": 10
          }
        },
        "bc0c907b890345fb8e25c30e91adcfdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53af87cf0e0e4af5950c68874cdef435",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3fe3629c306e4d1b8bceb9f979ddc3d5",
            "value": "‚Äá10/10‚Äá[01:21&lt;00:00,‚Äá‚Äá8.16s/it]"
          }
        },
        "93ad48eb2d714e9790500cd7cd749fad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e21fe6af4474428a91637bcfc258312": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca07bb3ee20a4afeb554b5bcdbfc1b6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfea2a09502a4db3acda9c19360ab8ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "392ad330efb14c419a641c1bc8f43cc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53af87cf0e0e4af5950c68874cdef435": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fe3629c306e4d1b8bceb9f979ddc3d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7adc20e6d55542c483ae2da5a98c587e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_add56e5405114eb89fe02471c57e7899",
              "IPY_MODEL_d758865bf2664c6ea31248cc843974f9",
              "IPY_MODEL_3ca11549641e47aa99975574a9f285bb"
            ],
            "layout": "IPY_MODEL_328a7757d034423cb8394715a90b3e4c"
          }
        },
        "add56e5405114eb89fe02471c57e7899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_296878d548f844ba8bae2c300354f707",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_37c9514871e64a68a2e7e075c31804ba",
            "value": "100%"
          }
        },
        "d758865bf2664c6ea31248cc843974f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3cfab2ffeee45589c8283c1b4abe12c",
            "max": 150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfaadaf997e14cf09c517d7ba8775601",
            "value": 150
          }
        },
        "3ca11549641e47aa99975574a9f285bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1b6a2273be944399bbf661b37812fd8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9a2e09eeafa3440a9b5b5f356b00f14d",
            "value": "‚Äá150/150‚Äá[00:15&lt;00:00,‚Äá10.68it/s]"
          }
        },
        "328a7757d034423cb8394715a90b3e4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "296878d548f844ba8bae2c300354f707": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37c9514871e64a68a2e7e075c31804ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3cfab2ffeee45589c8283c1b4abe12c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfaadaf997e14cf09c517d7ba8775601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1b6a2273be944399bbf661b37812fd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a2e09eeafa3440a9b5b5f356b00f14d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bce3cafd24147b19f5f0dce1642ae44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15fb718b8ed14bb183f26cf5f08d4fc8",
              "IPY_MODEL_45f7ef58c26b4ddc856e93a4d4367cec",
              "IPY_MODEL_4508699204cc434286c1fa917e8090dc"
            ],
            "layout": "IPY_MODEL_d58d29251ebb4e93b4d659e94150e487"
          }
        },
        "15fb718b8ed14bb183f26cf5f08d4fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c29a2066a97e45a783d064608a5dc7e5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ad70dfc51554473f8053c093c64aefde",
            "value": "100%"
          }
        },
        "45f7ef58c26b4ddc856e93a4d4367cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0daddfb3531344e78dacd724aa2a225a",
            "max": 150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_329a75043fd44d078c4c4e67509c2c82",
            "value": 150
          }
        },
        "4508699204cc434286c1fa917e8090dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1f26247a8d64a8e9279c6cfcbed9402",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bf18eb40084743d68eb4383f2937bd03",
            "value": "‚Äá150/150‚Äá[00:57&lt;00:00,‚Äá‚Äá2.76it/s]"
          }
        },
        "d58d29251ebb4e93b4d659e94150e487": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c29a2066a97e45a783d064608a5dc7e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad70dfc51554473f8053c093c64aefde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0daddfb3531344e78dacd724aa2a225a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "329a75043fd44d078c4c4e67509c2c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1f26247a8d64a8e9279c6cfcbed9402": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf18eb40084743d68eb4383f2937bd03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}